{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna\n",
    "Optunaとは，Preferred Networks発の 機械学習におけるハイパーパラメータの自動最適化（チューニング）を行うフレームワークのこと。  \n",
    "目的関数に対して，適当なハイパーパラメータを使って評価を繰り返し，その目的関数が最小となる最適なハイパーパラメータを探し出す．\n",
    "\n",
    "\n",
    "Optunaを使ってハイパーパラメータの中で最も良いものを探索してみることを目標とする。\n",
    "\n",
    "今回は簡単な課題としてMNISTを用いてチューニングを実行してみる。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 目的関数の定義\n",
    "最小化する目的関数を設定しないといけないが、これはobjective()関数で定義する。これは引数としてtrialオブジェクトが必要\n",
    "\n",
    "    def objective(trial):\n",
    "\n",
    "        return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trialオブジェクトの定義\n",
    "\n",
    "- カテゴリの試行を行うパラメータ\n",
    "param1 = trial.suggest_categorical(name, choices)\n",
    "\n",
    "- 整数値の試行を行うパラメータ\n",
    "param2 = trial.suggest_int(name, low, high)\n",
    "\n",
    "- 連続値の試行を行うパラメータ\n",
    "param3 = trial.suggest_uniform(name, low, high)\n",
    "\n",
    "- 離散値の試行を行うパラメータ\n",
    "param4 = trial.suggest_discrete_uniform(name, low, high, q)\n",
    "\n",
    "- 対数値の試行を行うパラメータ\n",
    "param5 = trial.suggest_loguniform(name, low, high)  \n",
    "\n",
    "\n",
    "\n",
    "nameはstr型でありパラメータの名前を指定し，choicesはlist型であり複数のカテゴリ名の選択肢として提示する引数となる。low，highではパラメータの最小値と最大値を提示し、qによってその値間を試行する間隔を設定する。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Studyオブジェクトの定義\n",
    "\n",
    "studyオブジェクトに最適化の結果が表示される。\n",
    "\n",
    "    study=optuna.create_study()\n",
    "\n",
    "それにoptimizerメソッドを用いることで最適なハイパーパラメータを求めることができる。\n",
    "\n",
    "    study.optimizer(objective,n_trailas=100)\n",
    "\n",
    "optimizeメソッドの第1引数は目的関数であるobjective関数となり，第2引数は試行回数となる．studyオブジェクト内でtrialの処理が行われるため，optimizeメソッドを実行するだけで自動的に目的関数の最小値およびそのハイパーパラメータを探してくれる。\n",
    "\n",
    "最適化の結果は以下で見ることができる。\n",
    "\n",
    "- 最適化したハイパーパラメータの結果\n",
    "study.best_params\n",
    "\n",
    "- 最適化後の目的関数の値\n",
    "study.best_value\n",
    "\n",
    "- 全試行過程\n",
    "study.trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import mlflow\n",
    "from torchvision import transforms,datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCHSIZE = 128\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "dataloader_train=torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data/mnist',train=True,download=True,transform=transform),\n",
    "    batch_size=BATCHSIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataloader_valid=torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data/mnist/',train=False,download=True,transform=transform),\n",
    "    batch_size=BATCHSIZE,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bdr/anaconda3/envs/deep/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import optuna\n",
    "optuna.logging.disable_default_handler()\n",
    "\n",
    "\n",
    "#モデルの定義\n",
    "\n",
    "#入力画像の高さと幅，畳み込み層のカーネルサイズ\n",
    "in_height = 28\n",
    "in_width = 28\n",
    "kernel = 3\n",
    "class Net(nn.Module):\n",
    "  def __init__(self, trial, num_layer, mid_units, num_filters):\n",
    "    super(Net, self).__init__()\n",
    "    self.activation = get_activation(trial)\n",
    "    #第1層\n",
    "    self.convs = nn.ModuleList([nn.Conv2d(in_channels=1, out_channels=num_filters[0], kernel_size=3)])\n",
    "    self.out_height = in_height - kernel +1\n",
    "    self.out_width = in_width - kernel +1\n",
    "    #第2層以降\n",
    "    for i in range(1, num_layer):\n",
    "      self.convs.append(nn.Conv2d(in_channels=num_filters[i-1], out_channels=num_filters[i], kernel_size=3))\n",
    "      self.out_height = self.out_height - kernel + 1\n",
    "      self.out_width = self.out_width - kernel +1\n",
    "    #pooling層\n",
    "    self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "    self.out_height = int(self.out_height / 2)\n",
    "    self.out_width = int(self.out_width / 2)\n",
    "    #線形層\n",
    "    self.out_feature = self.out_height * self.out_width * num_filters[num_layer - 1]\n",
    "    self.fc1 = nn.Linear(in_features=self.out_feature, out_features=mid_units) \n",
    "    self.fc2 = nn.Linear(in_features=mid_units, out_features=10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    for i,l in enumerate(self.convs):\n",
    "      x = l(x)\n",
    "      x = self.activation(x)\n",
    "    x = self.pool(x)\n",
    "    x = x.view(-1, self.out_feature)\n",
    "    x = self.fc1(x)\n",
    "    x = self.fc2(x)\n",
    "    return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    return 1 - correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def get_optimizer(trial, model):\n",
    "    optimizer_names = ['Adam', 'MomentumSGD', 'rmsprop']\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', optimizer_names)\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-10, 1e-3)\n",
    "    if optimizer_name == optimizer_names[0]: \n",
    "        adam_lr = trial.suggest_loguniform('adam_lr', 1e-5, 1e-1)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=adam_lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == optimizer_names[1]:\n",
    "        momentum_sgd_lr = trial.suggest_loguniform('momentum_sgd_lr', 1e-5, 1e-1)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=momentum_sgd_lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer = optim.RMSprop(model.parameters())\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(trial):\n",
    "    activation_names = ['ReLU', 'ELU']\n",
    "    activation_name = trial.suggest_categorical('activation', activation_names)\n",
    "    if activation_name == activation_names[0]:\n",
    "        activation = F.relu\n",
    "    else:\n",
    "        activation = F.elu\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 10\n",
    "def objective(trial):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#畳み込み層の数\n",
    "    num_layer = trial.suggest_int('num_layer', 1, 3)\n",
    "\n",
    "#FC層のユニット数\n",
    "    mid_units = int(trial.suggest_discrete_uniform(\"mid_units\", 100, 500, 100))\n",
    "\n",
    "#各畳込み層のフィルタ数\n",
    "    num_filters = [int(trial.suggest_discrete_uniform(\"num_filter_\"+str(i), 16, 128, 16)) for i in range(num_layer)]\n",
    "\n",
    "    model = Net(trial, num_layer, mid_units, num_filters).to(device)\n",
    "    optimizer = get_optimizer(trial, model)\n",
    "\n",
    "    for step in range(EPOCH):\n",
    "        train(model, device, dataloader_train, optimizer)\n",
    "        error_rate = test(model, device, dataloader_valid)\n",
    "\n",
    "    return error_rate\n",
    "#目的関数の定義だが、Optunaは目的関数が小さくなるようにハイパーパラメータをチューニングするため\n",
    "# objective関数は認識率でなく誤り率error_rateを返すよう定義。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bdr/anaconda3/envs/deep/lib/python3.11/site-packages/optuna/study/study.py:393: FutureWarning: `n_jobs` argument has been deprecated in v2.7.0. This feature will be removed in v4.0.0. See https://github.com/optuna/optuna/releases/tag/v2.7.0.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "TRIAL_SIZE = 100\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=TRIAL_SIZE,n_jobs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_layer': 5, 'mid_units': 400.0, 'num_filter_0': 32.0, 'num_filter_1': 48.0, 'num_filter_2': 64.0, 'num_filter_3': 112.0, 'num_filter_4': 128.0, 'activation': 'ReLU', 'optimizer': 'Adam', 'weight_decay': 7.1955149991328544e-06, 'adam_lr': 0.0007119522124205308}\n"
     ]
    }
   ],
   "source": [
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "in_height=28\n",
    "in_width=28\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "\n",
    "        self.conv_layers=nn.Sequential()\n",
    "\n",
    "        if study.best_params['activation']=='ReLU':\n",
    "            Activation=nn.ReLU()\n",
    "        else:\n",
    "            Activation=nn.ELU()\n",
    "\n",
    "        self.conv_layers.add_module(nn.Conv2d(1,study.best_params['num_filter'+str(i)]),3)\n",
    "        self.conv_layers.add_module(nn.BatchNorm2d(study.best_params['num_filter'+str(i)]))\n",
    "        self.conv_layers.add_module(nn.AvgPool2d(2))\n",
    "        self.conv_layers.add_module(Activation)\n",
    "\n",
    "        self.in_height = in_height-2         \n",
    "        self.in_width = in_width-2\n",
    "        \n",
    "        for i in range(1,study.best_paemas['num_layer']):\n",
    "            self.conv_layers.add_module(f\"conv_{i+1}\",nn.Conv2d(study.best_params['num_filter'+str(i)],study.best_parmas['num_filter'+str(i+1)],3))\n",
    "            self.in_height = in_height-2         \n",
    "            self.in_width = in_width-2\n",
    "            self.conv_layers.add_module(f'batch_norm_{i+1}',nn.BatchNorm2d(study.bast_params['num_filter'+str(i+1)]))\n",
    "            self.conv_layers.add_module(f'relu_{i+1}',Activation)\n",
    "            self.conv_layers.add_module(nn.AvgPool2d(2))\n",
    "            self.out_height = int(self.out_height / 2)\n",
    "            self.out_width = int(self.out_width / 2)\n",
    "            \n",
    "        self.fc_layers=nn.Sequential(\n",
    "            nn.Linear(self.out_height * self.out_width * study.best_params['num_filters'+str(study.best_paemas['num_layer'])]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(study.best_params['num_filters'+str(study.best_paemas['num_layer'])],10)            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    'batch_size':100,\n",
    "    'epochs':100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'i' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m         torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39minit\u001b[39m.\u001b[39mkaiming_normal_(m\u001b[39m.\u001b[39mweight)\n\u001b[1;32m      4\u001b[0m         m\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill_(\u001b[39m0.0\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m model\u001b[39m=\u001b[39mCNN()\n\u001b[1;32m      7\u001b[0m model\u001b[39m.\u001b[39mapply(init_weights)\n\u001b[1;32m      9\u001b[0m batch_size \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[89], line 16\u001b[0m, in \u001b[0;36mCNN.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     Activation\u001b[39m=\u001b[39mnn\u001b[39m.\u001b[39mELU()\n\u001b[0;32m---> 16\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_layers\u001b[39m.\u001b[39madd_module(nn\u001b[39m.\u001b[39mConv2d(\u001b[39m1\u001b[39m,study\u001b[39m.\u001b[39mbest_params[\u001b[39m'\u001b[39m\u001b[39mnum_filter\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(i)]),\u001b[39m3\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_layers\u001b[39m.\u001b[39madd_module(nn\u001b[39m.\u001b[39mBatchNorm2d(study\u001b[39m.\u001b[39mbest_params[\u001b[39m'\u001b[39m\u001b[39mnum_filter\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(i)]))\n\u001b[1;32m     18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_layers\u001b[39m.\u001b[39madd_module(nn\u001b[39m.\u001b[39mAvgPool2d(\u001b[39m2\u001b[39m))\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'i' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "def init_weights(m):  # Heの初期化\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.0)\n",
    "\n",
    "model=CNN()\n",
    "model.apply(init_weights)\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "device = 'cuda'\n",
    "\n",
    "Model=model.to(device)\n",
    "optimizer = optim.Adam(Model.parameters(), lr=study.best_params['adam_lr'],weight_decay=study.best_param['weight_decay'])\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
