{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自己教師あり学習の一つの、MaskedAutoEncoderを実装する。\n",
    "- Encoderに入力画像の一部のパッチのみを入力して、DecodeはEncodeした潜在表現と、学習可能なmask tokenを入力することで元の画像を再構成すると言うもの\n",
    "<div align=\"center\">\n",
    "<img src=\"https://camo.qiitausercontent.com/958cb06c6dcabefd14d585cafd5213ea0d662684/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313731383637382f38656435333136322d343537392d336561312d306266652d3662356336326665393936662e706e67\" width=\"30%\">\n",
    "</div>\n",
    "\n",
    "1. トークンではなく、エンコードした表現からピクセルを復元させる(事前学習の前にトークナイザーをつくる必要がなくなる)\n",
    "1. マスクしたパッチをエンコードしない(その分計算が減る)\n",
    "\n",
    "参考にした記事 : https://qiita.com/soukun1988/items/c588d526c9dc12978ee2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from einops.layers.torch import Rearrange\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シード値の設定\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Transformer Blockの実装\n",
    "- 画像を取り扱う場合は、Encoder Blockのみで実装が可能\n",
    "- Encoder Blockの要素として(Multu-Head Attention , Feed-Forward Network)がある。\n",
    "- 画像をパッチに分割して系列データにするので、画像をパッチに分割をして埋め込みを行う Patch Embeddingも実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attentionの実装\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads, dim_head, dropout=0.):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        dim : int\n",
    "            入力データの次元数．埋め込み次元数と一致する．\n",
    "        heads : int\n",
    "            ヘッドの数．\n",
    "        dim_head : int\n",
    "            各ヘッドのデータの次元数．\n",
    "        dropout : float\n",
    "            Dropoutの確率(default=0.)．\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.dim_head = dim_head\n",
    "        inner_dim = dim_head * heads  # ヘッドに分割する前のQ, K, Vの次元数．self.dimと異なっても良い．\n",
    "        project_out = not (heads == 1 and dim_head == dim)  # headsが1，dim_headがdimと等しければ通常のSelf-Attention\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = math.sqrt(dim_head)  # ソフトマックス関数を適用する前のスケーリング係数(dim_k)\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)  # アテンションスコアの算出に利用するソフトマックス関数\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Q, K, Vに変換するための全結合層\n",
    "        self.to_q = nn.Linear(in_features=dim, out_features=inner_dim)\n",
    "        self.to_k = nn.Linear(in_features=dim, out_features=inner_dim)\n",
    "        self.to_v = nn.Linear(in_features=dim, out_features=inner_dim)\n",
    "\n",
    "        # dim != inner_dimなら線形層を入れる，そうでなければそのまま出力\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(in_features=inner_dim, out_features=dim),\n",
    "            nn.Dropout(dropout),\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        B: バッチサイズ\n",
    "        N: 系列長\n",
    "        D: データの次元数(dim)\n",
    "        \"\"\"\n",
    "        B, N, D = x.size()\n",
    "\n",
    "        # 入力データをQ, K, Vに変換する\n",
    "        # (B, N, dim) -> (B, N, inner_dim)\n",
    "        q = self.to_q(x)\n",
    "        k = self.to_k(x)\n",
    "        v = self.to_v(x)\n",
    "\n",
    "        # Q, K, Vをヘッドに分割する\n",
    "        # (B, N, inner_dim) -> (B, heads, N, dim_head)\n",
    "        q = rearrange(q, \"b n (h d) -> b h n d\", h=self.heads, d=self.dim_head)\n",
    "        k = rearrange(k, \"b n (h d) -> b h n d\", h=self.heads, d=self.dim_head)\n",
    "        v = rearrange(v, \"b n (h d) -> b h n d\", h=self.heads, d=self.dim_head)\n",
    "\n",
    "        # QK^T / sqrt(d_k)を計算する\n",
    "        # (B, heads, N, dim_head) x (B, heads, dim_head, N) -> (B, heads, N, N)\n",
    "        dots = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n",
    "\n",
    "        # ソフトマックス関数でスコアを算出し，Dropoutをする\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        # softmax(QK^T / sqrt(d_k))Vを計算する\n",
    "        # (B, heads, N, N) x (B, heads, N, dim_head) -> (B, heads, N, dim_head)\n",
    "        out = torch.matmul(attn ,v)\n",
    "\n",
    "        # もとの形に戻す\n",
    "        # (B, heads, N, dim_head) -> (B, N, dim)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\", h=self.heads, d=self.dim_head)\n",
    "\n",
    "        # 次元が違っていればもとに戻して出力\n",
    "        # 表現の可視化のためにattention mapも返すようにしておく\n",
    "        return self.to_out(out), attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed-Forward Networkの実装\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        dim : int\n",
    "            入力データの次元数．\n",
    "        hidden_dim : int\n",
    "            隠れ層の次元．\n",
    "        dropout : float\n",
    "            各全結合層の後のDropoutの確率(default=0.)．\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features=dim, out_features=hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features=hidden_dim, out_features=dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        (B, D) -> (B, D)\n",
    "        B: バッチサイズ\n",
    "        D: 次元数\n",
    "        \"\"\"\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer Blockの実装\n",
    "- Attention,FFNを用いてBlockを実装。TransformerはこのBlockが積み重なっているので、これを実装してしまうと後が楽。具体的な内容はViT.ipynbで"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, heads, dim_head, mlp_dim, dropout):\n",
    "        \"\"\"\n",
    "        TransformerのEncoder Blockの実装．\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        dim : int\n",
    "            埋め込みされた次元数．PatchEmbedのembed_dimと同じ値．\n",
    "        heads : int\n",
    "            Multi-Head Attentionのヘッドの数．\n",
    "        dim_head : int\n",
    "            Multi-Head Attentionの各ヘッドの次元数．\n",
    "        mlp_dim : int\n",
    "            Feed-Forward Networkの隠れ層の次元数．\n",
    "        dropout : float\n",
    "            Droptou層の確率p．\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn_ln = nn.LayerNorm(dim)  # Attention前のLayerNorm\n",
    "        self.attn = Attention(dim, heads, dim_head, dropout)\n",
    "        self.ffn_ln = nn.LayerNorm(dim)  # FFN前のLayerNorm\n",
    "        self.ffn = FFN(dim, mlp_dim, dropout)\n",
    "\n",
    "    def forward(self, x, return_attn=False):\n",
    "        \"\"\"\n",
    "        x: (B, N, dim)\n",
    "        B: バッチサイズ\n",
    "        N: 系列長\n",
    "        dim: 埋め込み次元\n",
    "        \"\"\"\n",
    "        y, attn = self.attn(self.attn_ln(x))\n",
    "        if return_attn:  # attention mapを返す（attention mapの可視化に利用）\n",
    "            return attn\n",
    "        x = y + x\n",
    "        out = self.ffn(self.ffn_ln(x)) + x\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch Embeddingの実装\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n",
    "        \"\"\"\n",
    "        入力画像をパッチごとに埋め込むための層．\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        image_size : Tuple[int]\n",
    "            入力画像のサイズ．\n",
    "        patch_size : Tuple[int]\n",
    "            各パッチのサイズ．\n",
    "        in_channels : int\n",
    "            入力画像のチャネル数．\n",
    "        embed_dim : int\n",
    "            埋め込み後の次元数．\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        image_height, image_width = image_size\n",
    "        patch_height, patch_width = patch_size\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, \"パッチサイズは，入力画像のサイズを割り切れる必要があります．\"\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)  # パッチの数\n",
    "        patch_dim = in_channels * patch_height * patch_width  # 各パッチを平坦化したときの次元数\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange(\"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1=patch_height, p2=patch_width),  # 画像をパッチに分割して平坦化\n",
    "            nn.Linear(in_features=patch_dim, out_features=embed_dim),  # 埋め込みを行う\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        B: バッチサイズ\n",
    "        C: 入力画像のチャネル数\n",
    "        H: 入力画像の高さ\n",
    "        W: 入力画像の幅\n",
    "        \"\"\"\n",
    "        return self.to_patch_embedding(x)  # (B, C, H, W) -> (B, num_patches, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked Autoencoder\n",
    "- MAEはAutoEncoderと同様の構造をしているので、EncoderとDecoderの部分に分けることができる。\n",
    "\n",
    "- Encoderでは、入力画像の一部のパッチをランダムに入力して処理をする\n",
    "- DecoderではEncoderからの出力と，mask tokenという特殊なトークンを用いて画像を再構成する．<br>\n",
    "このときにランダムに選択したパッチをもとの順番に戻して入力する必要がある。<br>\n",
    "またEncoderに入力されなかったパッチに対応する部分については，エンコードされた表現が存在しないため代わりにmask tokenを利用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_indexes(size):\n",
    "    \"\"\"\n",
    "    パッチをランダムに並べ替えるためのindexを生成する関数．\n",
    "\n",
    "    Argument\n",
    "    --------\n",
    "    size : int\n",
    "        入力されるパッチの数（系列長Nと同じ値）．\n",
    "    \"\"\"\n",
    "    forward_indexes = np.arange(size)  # 0からsizeまでを並べた配列を作成\n",
    "    np.random.shuffle(forward_indexes)  # 生成した配列をシャッフルすることで，パッチの順番をランダムに決定\n",
    "    backward_indexes = np.argsort(forward_indexes)  # 並べ替えたパッチをもとの順番に戻すためのidx\n",
    "\n",
    "    return forward_indexes, backward_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_indexes(sequences, indexes):\n",
    "    \"\"\"\n",
    "    パッチを並べ替えるための関数．\n",
    "\n",
    "    Argument\n",
    "    --------\n",
    "    sequences : torch.Tensor\n",
    "        入力画像をパッチ分割したデータ．(B, N, dim)の形状をしている．\n",
    "    indexes : np.ndarray\n",
    "        並べ替えるために利用するindex．\n",
    "        random_indexesで生成したforward_indexesかbackward_indexesが入ることが想定されている．\n",
    "    \"\"\"\n",
    "    return torch.gather(sequences, dim=1, index=indexes.unsqueeze(2).repeat(1, 1, sequences.shape[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchShuffle(nn.Module):\n",
    "    def __init__(self, ratio):\n",
    "        # ratio: Encoderに入力しないパッチの割合\n",
    "        super().__init__()\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def forward(self, patches):\n",
    "        \"\"\"\n",
    "        B: バッチサイズ\n",
    "        N: 系列長（＝パッチの数）\n",
    "        dim: 次元数（＝埋め込みの次元数）\n",
    "        \"\"\"\n",
    "        B, N, dim = patches.shape\n",
    "        remain_N = int(N * (1 - self.ratio))  # Encoderに入力するパッチの数\n",
    "\n",
    "        indexes = [random_indexes(N) for _ in range(B)]  # バッチごとに異なる順番のindexを作る\n",
    "        forward_indexes = torch.as_tensor(np.stack([i[0] for i in indexes], axis=-1), dtype=torch.long).T.to(patches.device)  # バッチを並べ替えるときのidx (B, N)\n",
    "        backward_indexes = torch.as_tensor(np.stack([i[1] for i in indexes], axis=-1), dtype=torch.long).T.to(patches.device)  # 並べ替えたパッチをもとの順番に戻すためのidx  (B, N)\n",
    "\n",
    "        patches = take_indexes(patches, forward_indexes)  # パッチを並べ替える\n",
    "        patches = patches[:, :remain_N, :]  # Encoderに入力するパッチを抽出\n",
    "\n",
    "        return patches, forward_indexes, backward_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EncoderとDecoderの処理\n",
    "#### Encoder\n",
    "1. 入力画像をパッチに分割して(PatchEmbedding)，positional embeddingする\n",
    "1. 分割したパッチをランダムに並べ替えて，必要なパッチのみにする(PatchShuffle)．\n",
    "1. パッチをEncoderに入力して表現を獲得する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAE_Encoder(torch.nn.Module):\n",
    "    def __init__(self, image_size=[32, 32], patch_size=[2, 2], emb_dim=192, num_layer=12,\n",
    "                 heads=3, dim_head=64, mlp_dim=192, mask_ratio=0.75, dropout=0.):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "\n",
    "        image_size : List[int]\n",
    "            入力画像の大きさ．\n",
    "        patch_size : List[int]\n",
    "            各パッチの大きさ．\n",
    "        emb_dim : int\n",
    "            データを埋め込む次元の数．\n",
    "        num_layer : int\n",
    "            Encoderに含まれるBlockの数．\n",
    "        heads : int\n",
    "            Multi-Head Attentionのヘッドの数．\n",
    "        dim_head : int\n",
    "            Multi-Head Attentionの各ヘッドの次元数．\n",
    "        mlp_dim : int\n",
    "            Feed-Forward Networkの隠れ層の次元数．\n",
    "        mask_ratio : float\n",
    "            入力パッチのマスクする割合．\n",
    "        dropout : float\n",
    "            ドロップアウトの確率．\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        img_height, img_width = image_size\n",
    "        patch_height, patch_width = patch_size\n",
    "        num_patches = (img_height // patch_height) * (img_width // patch_width)\n",
    "\n",
    "        self.cls_token = torch.nn.Parameter(torch.randn(1, 1, emb_dim))  # class tokenの初期化\n",
    "        self.pos_embedding = torch.nn.Parameter(torch.randn(1, num_patches, emb_dim))  # positional embedding（学習可能にしている）\n",
    "        self.shuffle = PatchShuffle(mask_ratio)\n",
    "\n",
    "        # 入力画像をパッチに分割する\n",
    "        self.patchify = PatchEmbedding(image_size, patch_size, 3, emb_dim)\n",
    "\n",
    "        # Encoder（Blockを重ねる）\n",
    "        self.transformer = torch.nn.Sequential(*[Block(emb_dim, heads, dim_head, mlp_dim, dropout) for _ in range(num_layer)])\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(emb_dim)\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        torch.nn.init.normal_(self.cls_token, std=0.02)\n",
    "        torch.nn.init.normal_(self.pos_embedding, std=0.02)\n",
    "\n",
    "    def forward(self, img):\n",
    "        # 1. 入力画像をパッチに分割して，positional embeddingする\n",
    "        patches = self.patchify(img)\n",
    "        patches = patches + self.pos_embedding\n",
    "\n",
    "        # 2. 分割したパッチをランダムに並べ替えて，必要なパッチのみ得る\n",
    "        patches, forward_indexes, backward_indexes = self.shuffle(patches)\n",
    "\n",
    "        # class tokenを結合\n",
    "        patches = torch.cat([self.cls_token.repeat(patches.shape[0], 1, 1), patches], dim=1)\n",
    "\n",
    "        # 3. Encoderで入力データを処理する\n",
    "        features = self.layer_norm(self.transformer(patches))\n",
    "\n",
    "        return features, backward_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder\n",
    "1. Encoderの入力にmask tokenを結合してからもとの順番に並べ替え，positional embeddingする．\n",
    "1. Decoderで得られた表現から元画像を再構成する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAE_Decoder(nn.Module):\n",
    "    def __init__(self, image_size=[32, 32], patch_size=[2, 2], emb_dim=192, num_layer=4,\n",
    "                 heads=3, dim_head=64, mlp_dim=192, dropout=0.):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "\n",
    "        image_size : List[int]\n",
    "            入力画像の大きさ．\n",
    "        patch_size : List[int]\n",
    "            各パッチの大きさ．\n",
    "        emb_dim : int\n",
    "            データを埋め込む次元の数．\n",
    "        num_layer : int\n",
    "            Decoderに含まれるBlockの数．\n",
    "        heads : int\n",
    "            Multi-Head Attentionのヘッドの数．\n",
    "        dim_head : int\n",
    "            Multi-Head Attentionの各ヘッドの次元数．\n",
    "        mlp_dim : int\n",
    "            Feed-Forward Networkの隠れ層の次元数．\n",
    "        dropout : float\n",
    "            ドロップアウトの確率．\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        img_height, img_width = image_size\n",
    "        patch_height, patch_width = patch_size\n",
    "        num_patches = (img_height // patch_height) * (img_width // patch_width)\n",
    "\n",
    "        self.mask_token = torch.nn.Parameter(torch.rand(1, 1, emb_dim))\n",
    "        self.pos_embedding = torch.nn.Parameter(torch.rand(1, num_patches+1, emb_dim))\n",
    "\n",
    "        # Decoder(Blockを重ねる）\n",
    "        self.transformer = torch.nn.Sequential(*[Block(emb_dim, heads, dim_head, mlp_dim, dropout) for _ in range(num_layer)])\n",
    "\n",
    "        # 埋め込みされた表現から画像を復元するためのhead\n",
    "        self.head = torch.nn.Linear(emb_dim, 3 * patch_height * patch_width)\n",
    "        # (B, N, dim)から(B, C, H, W)にreshapeするためのインスタンス\n",
    "        self.patch2img = Rearrange(\"b (h w) (c p1 p2) -> b c (h p1) (w p2)\", p1=patch_height, p2=patch_width, h=img_height // patch_height)\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        torch.nn.init.normal_(self.mask_token, std=0.02)\n",
    "        torch.nn.init.normal_(self.pos_embedding, std=0.02)\n",
    "\n",
    "    def forward(self, features, backward_indexes):\n",
    "        # 系列長\n",
    "        T = features.shape[1]\n",
    "\n",
    "        # class tokenがある分backward_indexesの最初に0を追加する\n",
    "        # .toはデバイスの変更でよく利用するが，tensorを渡すことでdtypeを変えることができる\n",
    "        backward_indexes = torch.cat([torch.zeros(backward_indexes.shape[0], 1).to(backward_indexes), backward_indexes+1], dim=1)\n",
    "\n",
    "        # 1. mask_tokenを結合して並べ替える．\n",
    "        # (B, N*(1-mask_ratio)+1, dim) -> (B, N+1, dim)\n",
    "        features = torch.cat([features, self.mask_token.repeat(features.shape[0], backward_indexes.shape[1] - features.shape[1], 1)], dim=1)\n",
    "        features = take_indexes(features, backward_indexes)\n",
    "        features = features + self.pos_embedding\n",
    "\n",
    "        features = self.transformer(features)\n",
    "\n",
    "        # class tokenを除去する\n",
    "        # (B, N+1, dim) -> (B, N, dim)\n",
    "        features = features[:, 1:, :]\n",
    "\n",
    "        # 2. 画像を再構成する．\n",
    "        # (B, N, dim) -> (B, N, 3 * patch_height * patch_width)\n",
    "        patches = self.head(features)\n",
    "\n",
    "        # MAEではマスクした部分でのみ損失関数を計算するため，maskも一緒に返す\n",
    "        mask = torch.zeros_like(patches)\n",
    "        mask[:, T-1:] = 1  # cls tokenを含めていた分ずらしている\n",
    "        mask = take_indexes(mask, backward_indexes[:, 1:] - 1)\n",
    "\n",
    "        img = self.patch2img(patches)\n",
    "        mask = self.patch2img(mask)\n",
    "\n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAE_ViT(torch.nn.Module):\n",
    "    def __init__(self, image_size=[32, 32], patch_size=[2, 2], emb_dim=192,\n",
    "                 enc_layers=12, enc_heads=3, enc_dim_head=64, enc_mlp_dim=768,\n",
    "                 dec_layers=4, dec_heads=3, dec_dim_head=64, dec_mlp_dim=768,\n",
    "                 mask_ratio=0.75, dropout=0.):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        image_size : List[int]\n",
    "            入力画像の大きさ．\n",
    "        patch_size : List[int]\n",
    "            各パッチの大きさ．\n",
    "        emb_dim : int\n",
    "            データを埋め込む次元の数．\n",
    "        {enc/dec}_layers : int\n",
    "            Encoder / Decoderに含まれるBlockの数．\n",
    "        {enc/dec}_heads : int\n",
    "            Encoder / DecoderのMulti-Head Attentionのヘッドの数．\n",
    "        {enc/dec}_dim_head : int\n",
    "            Encoder / DecoderのMulti-Head Attentionの各ヘッドの次元数．\n",
    "        {enc/dec}_mlp_dim : int\n",
    "            Encoder / DecoderのFeed-Forward Networkの隠れ層の次元数．\n",
    "        mask_ratio : float\n",
    "            入力パッチのマスクする割合．\n",
    "        dropout : float\n",
    "            ドロップアウトの確率．\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = MAE_Encoder(image_size, patch_size, emb_dim, enc_layers,\n",
    "                                   enc_heads, enc_dim_head, enc_mlp_dim, mask_ratio, dropout)\n",
    "        self.decoder = MAE_Decoder(image_size, patch_size, emb_dim, dec_layers,\n",
    "                                   dec_heads, dec_dim_head, dec_mlp_dim, dropout)\n",
    "\n",
    "    def forward(self, img):\n",
    "        features, backward_indexes = self.encoder(img)\n",
    "        rec_img, mask = self.decoder(features, backward_indexes)\n",
    "        return rec_img, mask\n",
    "\n",
    "    def get_last_selfattention(self, x):\n",
    "        patches = self.encoder.patchify(x)\n",
    "        patches = patches + self.encoder.pos_embedding\n",
    "\n",
    "        patches = torch.cat([self.encoder.cls_token.repeat(patches.shape[0], 1, 1), patches], dim=1)  # class tokenを結合\n",
    "        for i, block in enumerate(self.encoder.transformer):\n",
    "            if i < len(self.encoder.transformer) - 1:\n",
    "                patches = block(patches)\n",
    "            else:\n",
    "                return block(patches, return_attn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAEの学習では学習率のスケジューラにcosine schedulerの使用と，warmupを用いる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine scheduler\n",
    "class CosineScheduler:\n",
    "    def __init__(self, epochs, lr, warmup_length=5):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        epochs : int\n",
    "            学習のエポック数．\n",
    "        lr : float\n",
    "            学習率．\n",
    "        warmup_length : int\n",
    "            warmupを適用するエポック数．\n",
    "        \"\"\"\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.warmup = warmup_length\n",
    "\n",
    "    def __call__(self, epoch):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        epoch : int\n",
    "            現在のエポック数．\n",
    "        \"\"\"\n",
    "        progress = (epoch - self.warmup) / (self.epochs - self.warmup+1e-5)\n",
    "        progress = np.clip(progress, 0.0, 1.0)\n",
    "        lr = self.lr * 0.5 * (1. + np.cos(np.pi * progress))\n",
    "\n",
    "        if self.warmup:\n",
    "            lr = lr * min(1., (epoch+1) / self.warmup)\n",
    "\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_lr(lr, optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパーパラメータの設定\n",
    "config = {\n",
    "    \"image_size\": [32, 32],\n",
    "    \"patch_size\": [2, 2],\n",
    "    \"emb_dim\": 192,\n",
    "    \"enc_layers\": 12,\n",
    "    \"enc_heads\": 3,\n",
    "    \"enc_dim_head\": 64,\n",
    "    \"enc_mlp_dim\": 192,\n",
    "    \"dec_layers\": 4,\n",
    "    \"dec_heads\": 3,\n",
    "    \"dec_dim_head\": 64,\n",
    "    \"dec_mlp_dim\": 192,\n",
    "    \"mask_ratio\": 0.75,\n",
    "    \"dropout\": 0.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの定義\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = MAE_ViT(**config).to(device)\n",
    "\n",
    "epochs = 200\n",
    "lr = 0.0024\n",
    "warmup_length = 200\n",
    "batch_size = 512\n",
    "step_count = 0\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.95), weight_decay=0.05)\n",
    "scheduler = CosineScheduler(epochs, lr, warmup_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize(0.5, 0.5)]\n",
    ")\n",
    "valid_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize(0.5, 0.5)]\n",
    ")\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(\n",
    "        \"./\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=train_transform,\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "valid_dl = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(\n",
    "        \"./\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=valid_transform,\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1 / 200] Train Loss: 0.2586 Valid Loss: 0.2121\n",
      "Epoch[2 / 200] Train Loss: 0.2002 Valid Loss: 0.1911\n",
      "Epoch[3 / 200] Train Loss: 0.1883 Valid Loss: 0.1854\n",
      "Epoch[4 / 200] Train Loss: 0.1838 Valid Loss: 0.1823\n",
      "Epoch[5 / 200] Train Loss: 0.1817 Valid Loss: 0.1811\n",
      "Epoch[6 / 200] Train Loss: 0.1804 Valid Loss: 0.1796\n",
      "Epoch[7 / 200] Train Loss: 0.1794 Valid Loss: 0.1774\n",
      "Epoch[8 / 200] Train Loss: 0.1767 Valid Loss: 0.1746\n",
      "Epoch[9 / 200] Train Loss: 0.1736 Valid Loss: 0.1718\n",
      "Epoch[10 / 200] Train Loss: 0.1723 Valid Loss: 0.1709\n",
      "Epoch[11 / 200] Train Loss: 0.1701 Valid Loss: 0.1682\n",
      "Epoch[12 / 200] Train Loss: 0.1680 Valid Loss: 0.1644\n",
      "Epoch[13 / 200] Train Loss: 0.1634 Valid Loss: 0.1635\n",
      "Epoch[14 / 200] Train Loss: 0.1618 Valid Loss: 0.1601\n",
      "Epoch[15 / 200] Train Loss: 0.1590 Valid Loss: 0.1575\n",
      "Epoch[16 / 200] Train Loss: 0.1580 Valid Loss: 0.1554\n",
      "Epoch[17 / 200] Train Loss: 0.1551 Valid Loss: 0.1530\n",
      "Epoch[18 / 200] Train Loss: 0.1533 Valid Loss: 0.1521\n",
      "Epoch[19 / 200] Train Loss: 0.1502 Valid Loss: 0.1492\n",
      "Epoch[20 / 200] Train Loss: 0.1475 Valid Loss: 0.1447\n",
      "Epoch[21 / 200] Train Loss: 0.1453 Valid Loss: 0.1431\n",
      "Epoch[22 / 200] Train Loss: 0.1446 Valid Loss: 0.1446\n",
      "Epoch[23 / 200] Train Loss: 0.1432 Valid Loss: 0.1456\n",
      "Epoch[24 / 200] Train Loss: 0.1426 Valid Loss: 0.1403\n",
      "Epoch[25 / 200] Train Loss: 0.1406 Valid Loss: 0.1439\n",
      "Epoch[26 / 200] Train Loss: 0.1407 Valid Loss: 0.1391\n",
      "Epoch[27 / 200] Train Loss: 0.1398 Valid Loss: 0.1387\n",
      "Epoch[28 / 200] Train Loss: 0.1378 Valid Loss: 0.1352\n",
      "Epoch[29 / 200] Train Loss: 0.1360 Valid Loss: 0.1322\n",
      "Epoch[30 / 200] Train Loss: 0.1386 Valid Loss: 0.1382\n",
      "Epoch[31 / 200] Train Loss: 0.1356 Valid Loss: 0.1314\n",
      "Epoch[32 / 200] Train Loss: 0.1324 Valid Loss: 0.1289\n",
      "Epoch[33 / 200] Train Loss: 0.1338 Valid Loss: 0.1333\n",
      "Epoch[34 / 200] Train Loss: 0.1301 Valid Loss: 0.1280\n",
      "Epoch[35 / 200] Train Loss: 0.1278 Valid Loss: 0.1278\n",
      "Epoch[36 / 200] Train Loss: 0.1281 Valid Loss: 0.1279\n",
      "Epoch[37 / 200] Train Loss: 0.1264 Valid Loss: 0.1241\n",
      "Epoch[38 / 200] Train Loss: 0.1254 Valid Loss: 0.1258\n",
      "Epoch[39 / 200] Train Loss: 0.1255 Valid Loss: 0.1236\n",
      "Epoch[40 / 200] Train Loss: 0.1245 Valid Loss: 0.1219\n",
      "Epoch[41 / 200] Train Loss: 0.1225 Valid Loss: 0.1204\n",
      "Epoch[42 / 200] Train Loss: 0.1219 Valid Loss: 0.1345\n",
      "Epoch[43 / 200] Train Loss: 0.1245 Valid Loss: 0.1207\n",
      "Epoch[44 / 200] Train Loss: 0.1201 Valid Loss: 0.1218\n",
      "Epoch[45 / 200] Train Loss: 0.1185 Valid Loss: 0.1187\n",
      "Epoch[46 / 200] Train Loss: 0.1172 Valid Loss: 0.1151\n",
      "Epoch[47 / 200] Train Loss: 0.1141 Valid Loss: 0.1146\n",
      "Epoch[48 / 200] Train Loss: 0.1156 Valid Loss: 0.1131\n",
      "Epoch[49 / 200] Train Loss: 0.1108 Valid Loss: 0.1076\n",
      "Epoch[50 / 200] Train Loss: 0.1135 Valid Loss: 0.1204\n",
      "Epoch[51 / 200] Train Loss: 0.1107 Valid Loss: 0.1078\n",
      "Epoch[52 / 200] Train Loss: 0.1053 Valid Loss: 0.1022\n",
      "Epoch[53 / 200] Train Loss: 0.1064 Valid Loss: 0.1072\n",
      "Epoch[54 / 200] Train Loss: 0.1025 Valid Loss: 0.0995\n",
      "Epoch[55 / 200] Train Loss: 0.1012 Valid Loss: 0.0952\n",
      "Epoch[56 / 200] Train Loss: 0.0932 Valid Loss: 0.0927\n",
      "Epoch[57 / 200] Train Loss: 0.0898 Valid Loss: 0.0858\n",
      "Epoch[58 / 200] Train Loss: 0.0786 Valid Loss: 0.0756\n",
      "Epoch[59 / 200] Train Loss: 0.0737 Valid Loss: 0.0739\n",
      "Epoch[60 / 200] Train Loss: 0.0717 Valid Loss: 0.0694\n",
      "Epoch[61 / 200] Train Loss: 0.0721 Valid Loss: 0.0709\n",
      "Epoch[62 / 200] Train Loss: 0.0699 Valid Loss: 0.0683\n",
      "Epoch[63 / 200] Train Loss: 0.0687 Valid Loss: 0.0691\n",
      "Epoch[64 / 200] Train Loss: 0.0682 Valid Loss: 0.0663\n",
      "Epoch[65 / 200] Train Loss: 0.0666 Valid Loss: 0.0654\n",
      "Epoch[66 / 200] Train Loss: 0.0663 Valid Loss: 0.0645\n",
      "Epoch[67 / 200] Train Loss: 0.0653 Valid Loss: 0.0646\n",
      "Epoch[68 / 200] Train Loss: 0.0634 Valid Loss: 0.0623\n",
      "Epoch[69 / 200] Train Loss: 0.0649 Valid Loss: 0.0642\n",
      "Epoch[70 / 200] Train Loss: 0.0624 Valid Loss: 0.0613\n",
      "Epoch[71 / 200] Train Loss: 0.0620 Valid Loss: 0.0619\n",
      "Epoch[72 / 200] Train Loss: 0.0601 Valid Loss: 0.0609\n",
      "Epoch[73 / 200] Train Loss: 0.0606 Valid Loss: 0.0585\n",
      "Epoch[74 / 200] Train Loss: 0.0596 Valid Loss: 0.0592\n",
      "Epoch[75 / 200] Train Loss: 0.0577 Valid Loss: 0.0594\n",
      "Epoch[76 / 200] Train Loss: 0.0586 Valid Loss: 0.0561\n",
      "Epoch[77 / 200] Train Loss: 0.0574 Valid Loss: 0.0564\n",
      "Epoch[78 / 200] Train Loss: 0.0562 Valid Loss: 0.0551\n",
      "Epoch[79 / 200] Train Loss: 0.0559 Valid Loss: 0.0549\n",
      "Epoch[80 / 200] Train Loss: 0.0550 Valid Loss: 0.0545\n",
      "Epoch[81 / 200] Train Loss: 0.0573 Valid Loss: 0.0577\n",
      "Epoch[82 / 200] Train Loss: 0.0551 Valid Loss: 0.0535\n",
      "Epoch[83 / 200] Train Loss: 0.0551 Valid Loss: 0.0544\n",
      "Epoch[84 / 200] Train Loss: 0.0534 Valid Loss: 0.0552\n",
      "Epoch[85 / 200] Train Loss: 0.0558 Valid Loss: 0.0542\n",
      "Epoch[86 / 200] Train Loss: 0.0530 Valid Loss: 0.0527\n",
      "Epoch[87 / 200] Train Loss: 0.0544 Valid Loss: 0.0531\n",
      "Epoch[88 / 200] Train Loss: 0.0521 Valid Loss: 0.0525\n",
      "Epoch[89 / 200] Train Loss: 0.0527 Valid Loss: 0.0508\n",
      "Epoch[90 / 200] Train Loss: 0.0522 Valid Loss: 0.0519\n",
      "Epoch[91 / 200] Train Loss: 0.0526 Valid Loss: 0.0515\n",
      "Epoch[92 / 200] Train Loss: 0.0508 Valid Loss: 0.0513\n",
      "Epoch[93 / 200] Train Loss: 0.0515 Valid Loss: 0.0502\n",
      "Epoch[94 / 200] Train Loss: 0.0510 Valid Loss: 0.0520\n",
      "Epoch[95 / 200] Train Loss: 0.0503 Valid Loss: 0.0494\n",
      "Epoch[96 / 200] Train Loss: 0.0506 Valid Loss: 0.0507\n",
      "Epoch[97 / 200] Train Loss: 0.0502 Valid Loss: 0.0503\n",
      "Epoch[98 / 200] Train Loss: 0.0492 Valid Loss: 0.0495\n",
      "Epoch[99 / 200] Train Loss: 0.0509 Valid Loss: 0.0496\n",
      "Epoch[100 / 200] Train Loss: 0.0492 Valid Loss: 0.0501\n",
      "Epoch[101 / 200] Train Loss: 0.0487 Valid Loss: 0.0477\n",
      "Epoch[102 / 200] Train Loss: 0.0505 Valid Loss: 0.0509\n",
      "Epoch[103 / 200] Train Loss: 0.0498 Valid Loss: 0.0482\n",
      "Epoch[104 / 200] Train Loss: 0.0485 Valid Loss: 0.0511\n",
      "Epoch[105 / 200] Train Loss: 0.0490 Valid Loss: 0.0472\n",
      "Epoch[106 / 200] Train Loss: 0.0482 Valid Loss: 0.0466\n",
      "Epoch[107 / 200] Train Loss: 0.0477 Valid Loss: 0.0481\n",
      "Epoch[108 / 200] Train Loss: 0.0480 Valid Loss: 0.0465\n",
      "Epoch[109 / 200] Train Loss: 0.0481 Valid Loss: 0.0480\n",
      "Epoch[110 / 200] Train Loss: 0.0476 Valid Loss: 0.0459\n",
      "Epoch[111 / 200] Train Loss: 0.0461 Valid Loss: 0.0475\n",
      "Epoch[112 / 200] Train Loss: 0.0471 Valid Loss: 0.0470\n",
      "Epoch[113 / 200] Train Loss: 0.0463 Valid Loss: 0.0461\n",
      "Epoch[114 / 200] Train Loss: 0.0460 Valid Loss: 0.0451\n",
      "Epoch[115 / 200] Train Loss: 0.0462 Valid Loss: 0.0459\n",
      "Epoch[116 / 200] Train Loss: 0.0462 Valid Loss: 0.0452\n",
      "Epoch[117 / 200] Train Loss: 0.0453 Valid Loss: 0.0446\n",
      "Epoch[118 / 200] Train Loss: 0.0450 Valid Loss: 0.0451\n",
      "Epoch[119 / 200] Train Loss: 0.0460 Valid Loss: 0.0457\n",
      "Epoch[120 / 200] Train Loss: 0.0447 Valid Loss: 0.0461\n",
      "Epoch[121 / 200] Train Loss: 0.0456 Valid Loss: 0.0445\n",
      "Epoch[122 / 200] Train Loss: 0.0449 Valid Loss: 0.0464\n",
      "Epoch[123 / 200] Train Loss: 0.0452 Valid Loss: 0.0444\n",
      "Epoch[124 / 200] Train Loss: 0.0443 Valid Loss: 0.0447\n",
      "Epoch[125 / 200] Train Loss: 0.0446 Valid Loss: 0.0437\n",
      "Epoch[126 / 200] Train Loss: 0.0439 Valid Loss: 0.0466\n",
      "Epoch[127 / 200] Train Loss: 0.0441 Valid Loss: 0.0476\n",
      "Epoch[128 / 200] Train Loss: 0.0446 Valid Loss: 0.0433\n",
      "Epoch[129 / 200] Train Loss: 0.0434 Valid Loss: 0.0431\n",
      "Epoch[130 / 200] Train Loss: 0.0460 Valid Loss: 0.0441\n",
      "Epoch[131 / 200] Train Loss: 0.0432 Valid Loss: 0.0432\n",
      "Epoch[132 / 200] Train Loss: 0.0433 Valid Loss: 0.0425\n",
      "Epoch[133 / 200] Train Loss: 0.0442 Valid Loss: 0.0435\n",
      "Epoch[134 / 200] Train Loss: 0.0436 Valid Loss: 0.0424\n",
      "Epoch[135 / 200] Train Loss: 0.0427 Valid Loss: 0.0418\n",
      "Epoch[136 / 200] Train Loss: 0.0436 Valid Loss: 0.0433\n",
      "Epoch[137 / 200] Train Loss: 0.0435 Valid Loss: 0.0430\n",
      "Epoch[138 / 200] Train Loss: 0.0424 Valid Loss: 0.0433\n",
      "Epoch[139 / 200] Train Loss: 0.0425 Valid Loss: 0.0415\n",
      "Epoch[140 / 200] Train Loss: 0.0456 Valid Loss: 0.0456\n",
      "Epoch[141 / 200] Train Loss: 0.0432 Valid Loss: 0.0418\n",
      "Epoch[142 / 200] Train Loss: 0.0414 Valid Loss: 0.0418\n",
      "Epoch[143 / 200] Train Loss: 0.0438 Valid Loss: 0.0425\n",
      "Epoch[144 / 200] Train Loss: 0.0420 Valid Loss: 0.0412\n",
      "Epoch[145 / 200] Train Loss: 0.0421 Valid Loss: 0.0422\n",
      "Epoch[146 / 200] Train Loss: 0.0416 Valid Loss: 0.0414\n",
      "Epoch[147 / 200] Train Loss: 0.0422 Valid Loss: 0.0465\n",
      "Epoch[148 / 200] Train Loss: 0.0451 Valid Loss: 0.0434\n",
      "Epoch[149 / 200] Train Loss: 0.0418 Valid Loss: 0.0410\n",
      "Epoch[150 / 200] Train Loss: 0.0418 Valid Loss: 0.0410\n",
      "Epoch[151 / 200] Train Loss: 0.0414 Valid Loss: 0.0418\n",
      "Epoch[152 / 200] Train Loss: 0.0416 Valid Loss: 0.0405\n",
      "Epoch[153 / 200] Train Loss: 0.0414 Valid Loss: 0.0421\n",
      "Epoch[154 / 200] Train Loss: 0.0442 Valid Loss: 0.0426\n",
      "Epoch[155 / 200] Train Loss: 0.0412 Valid Loss: 0.0406\n",
      "Epoch[156 / 200] Train Loss: 0.0400 Valid Loss: 0.0406\n",
      "Epoch[157 / 200] Train Loss: 0.0427 Valid Loss: 0.0418\n",
      "Epoch[158 / 200] Train Loss: 0.0416 Valid Loss: 0.0408\n",
      "Epoch[159 / 200] Train Loss: 0.0399 Valid Loss: 0.0395\n",
      "Epoch[160 / 200] Train Loss: 0.0433 Valid Loss: 0.0427\n",
      "Epoch[161 / 200] Train Loss: 0.0412 Valid Loss: 0.0402\n",
      "Epoch[162 / 200] Train Loss: 0.0405 Valid Loss: 0.0420\n",
      "Epoch[163 / 200] Train Loss: 0.0402 Valid Loss: 0.0402\n",
      "Epoch[164 / 200] Train Loss: 0.0406 Valid Loss: 0.0397\n",
      "Epoch[165 / 200] Train Loss: 0.0402 Valid Loss: 0.0398\n",
      "Epoch[166 / 200] Train Loss: 0.0396 Valid Loss: 0.0405\n",
      "Epoch[167 / 200] Train Loss: 0.0422 Valid Loss: 0.0410\n",
      "Epoch[168 / 200] Train Loss: 0.0417 Valid Loss: 0.0423\n",
      "Epoch[169 / 200] Train Loss: 0.0408 Valid Loss: 0.0396\n",
      "Epoch[170 / 200] Train Loss: 0.0413 Valid Loss: 0.0404\n",
      "Epoch[171 / 200] Train Loss: 0.0395 Valid Loss: 0.0395\n",
      "Epoch[172 / 200] Train Loss: 0.0400 Valid Loss: 0.0389\n",
      "Epoch[173 / 200] Train Loss: 0.0395 Valid Loss: 0.0405\n",
      "Epoch[174 / 200] Train Loss: 0.0408 Valid Loss: 0.0415\n",
      "Epoch[175 / 200] Train Loss: 0.0400 Valid Loss: 0.0395\n",
      "Epoch[176 / 200] Train Loss: 0.0399 Valid Loss: 0.0399\n",
      "Epoch[177 / 200] Train Loss: 0.0391 Valid Loss: 0.0385\n",
      "Epoch[178 / 200] Train Loss: 0.0399 Valid Loss: 0.0399\n",
      "Epoch[179 / 200] Train Loss: 0.0391 Valid Loss: 0.0387\n",
      "Epoch[180 / 200] Train Loss: 0.0400 Valid Loss: 0.0404\n",
      "Epoch[181 / 200] Train Loss: 0.0394 Valid Loss: 0.0388\n",
      "Epoch[182 / 200] Train Loss: 0.0392 Valid Loss: 0.0390\n",
      "Epoch[183 / 200] Train Loss: 0.0391 Valid Loss: 0.0387\n",
      "Epoch[184 / 200] Train Loss: 0.0422 Valid Loss: 0.0425\n",
      "Epoch[185 / 200] Train Loss: 0.0400 Valid Loss: 0.0393\n",
      "Epoch[186 / 200] Train Loss: 0.0396 Valid Loss: 0.0389\n",
      "Epoch[187 / 200] Train Loss: 0.0385 Valid Loss: 0.0384\n",
      "Epoch[188 / 200] Train Loss: 0.0384 Valid Loss: 0.0385\n",
      "Epoch[189 / 200] Train Loss: 0.0388 Valid Loss: 0.0402\n",
      "Epoch[190 / 200] Train Loss: 0.0389 Valid Loss: 0.0386\n",
      "Epoch[191 / 200] Train Loss: 0.0383 Valid Loss: 0.0383\n",
      "Epoch[192 / 200] Train Loss: 0.0390 Valid Loss: 0.0405\n",
      "Epoch[193 / 200] Train Loss: 0.0406 Valid Loss: 0.0390\n",
      "Epoch[194 / 200] Train Loss: 0.0387 Valid Loss: 0.0383\n",
      "Epoch[195 / 200] Train Loss: 0.0378 Valid Loss: 0.0379\n",
      "Epoch[196 / 200] Train Loss: 0.0399 Valid Loss: 0.0390\n",
      "Epoch[197 / 200] Train Loss: 0.0385 Valid Loss: 0.0379\n",
      "Epoch[198 / 200] Train Loss: 0.0380 Valid Loss: 0.0379\n",
      "Epoch[199 / 200] Train Loss: 0.0377 Valid Loss: 0.0393\n",
      "Epoch[200 / 200] Train Loss: 0.0400 Valid Loss: 0.0398\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # スケジューラで学習率を更新する\n",
    "    new_lr = scheduler(epoch)\n",
    "    set_lr(new_lr, optimizer)\n",
    "\n",
    "    total_train_loss = 0.\n",
    "    total_valid_loss = 0.\n",
    "\n",
    "    # モデルの訓練\n",
    "    for x, _ in train_dl:\n",
    "        step_count += 1\n",
    "        model.train()\n",
    "        x = x.to(device)\n",
    "\n",
    "        rec_img, mask = model(x)\n",
    "        train_loss = torch.mean((rec_img - x) ** 2 * mask) / config[\"mask_ratio\"]\n",
    "        train_loss.backward()\n",
    "\n",
    "        if step_count % 8 == 0:  # 8イテレーションごとに更新することで，擬似的にバッチサイズを大きくしている\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_train_loss += train_loss.item()\n",
    "\n",
    "    # モデルの評価\n",
    "    with torch.no_grad():\n",
    "        for x, _ in valid_dl:\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                x = x.to(device)\n",
    "\n",
    "                rec_img, mask = model(x)\n",
    "                valid_loss = torch.mean((rec_img - x) ** 2 * mask) / config[\"mask_ratio\"]\n",
    "\n",
    "                total_valid_loss += valid_loss.item()\n",
    "\n",
    "\n",
    "    print(f\"Epoch[{epoch+1} / {epochs}] Train Loss: {total_train_loss/len(train_dl):.4f} Valid Loss: {total_valid_loss/len(valid_dl):.4f}\")\n",
    "\n",
    "# モデルを保存しておく\n",
    "torch.save(model.state_dict(), \"model/MAE_pretrain_params.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
