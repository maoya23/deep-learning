{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vision Transformerの実装\n",
    "- 参考にした記事 https://qiita.com/zisui-sukitarou/items/d990a9630ff2c7f4abf2\n",
    "- 今回も例によってcifar-10によって実行する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vision transformernモデルを実装して理解することを目標にする。<br>\n",
    "<div align=\"center\">\n",
    "<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F585587%2F8ce3dea7-0287-85c6-4461-2a085185ed95.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=12ce97077cfddec29ed1d0ed55be3dd9\" width=\"40%\">\n",
    "</div>\n",
    "\n",
    "#### ViTとは\n",
    "- Transformerのエンコーダー部分を画像認識に応用したもの\n",
    "\n",
    "#### 特徴\n",
    "1. SoTAを上回る精度\n",
    "1. 畳み込みを行わないモデル\n",
    "1. それまでのSoTAよりも大幅に小さい計算コスト\n",
    "\n",
    "#### モデル概要\n",
    "1. 画像がパッチに分割される\n",
    "1. 各パッチがベクトルに変換される\n",
    "1. その先頭に[class]トークンを付与したものに位置エンコーディングが加算される\n",
    "1. それがTransformer Encoderによって処理される\n",
    "1. その出力の0番目のベクトルがMLP headで処理されてクラスが出力される\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from einops import repeat\n",
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    'BatchSize':128,\n",
    "    'seed':42,\n",
    "    'n_epochs' : 50,\n",
    "    'lr' : 0.001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Train data number:40000, Valid data number: 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainval_dataset = datasets.CIFAR10('../data/cifar10', train=True,download=True,transform=transforms.ToTensor())\n",
    "\n",
    "# 前処理を定義\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "trainval_dataset = datasets.CIFAR10('../data/cifar10', train=True, transform=transform)\n",
    "\n",
    "# trainとvalidに分割\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(trainval_dataset, [len(trainval_dataset)-10000, 10000],generator=torch.Generator().manual_seed(config['seed']))\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['BatchSize'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataloader_valid = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['BatchSize'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"Train data number:{}, Valid data number: {}\".format(len(train_dataset), len(val_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- H(image_size) : 画像の縦の長さ\n",
    "- W(image_size) : 画像の横の長さ（今回はH=W）\n",
    "- B(batch_size) : バッチサイズ\n",
    "- P(patch_size) : パッチサイズ（縦の長さと、横の長さ）\n",
    "- C(channels) : チャンネル数（RGBの場合C=3）\n",
    "- D(dim) : パッチベクトル変数後のベクトルの長さ\n",
    "- N(n_patches) : パッチの数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, n_classes, dim, depth, n_heads, channels = 3, mlp_dim = 256):\n",
    "        \"\"\" [input]\n",
    "            - image_size (int) : 画像の縦の長さ（= 横の長さ）\n",
    "            - patch_size (int) : パッチの縦の長さ（= 横の長さ）\n",
    "            - n_classes (int) : 分類するクラスの数\n",
    "            - dim (int) : 各パッチのベクトルが変換されたベクトルの長さ（参考[1] (1)式 D）\n",
    "            - depth (int) : Transformer Encoder の層の深さ（参考[1] (2)式 L）\n",
    "            - n_heads (int) : Multi-Head Attention の head の数\n",
    "            - chahnnels (int) : 入力のチャネル数（RGBの画像なら3）\n",
    "            - mlp_dim (int) : MLP の隠れ層のノード数\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        # Params\n",
    "        n_patches = (image_size // patch_size) ** 2\n",
    "        patch_dim = channels * patch_size * patch_size\n",
    "        self.depth = depth\n",
    "\n",
    "        # Layers\n",
    "        self.patching = Patching(patch_size = patch_size)\n",
    "        self.linear_projection_of_flattened_patches = LinearProjection(patch_dim = patch_dim, dim = dim)\n",
    "        self.embedding = Embedding(dim = dim, n_patches = n_patches)\n",
    "        self.transformer_encoder = TransformerEncoder(dim = dim, n_heads = n_heads, mlp_dim = mlp_dim, depth = depth)\n",
    "        self.mlp_head = MLPHead(dim = dim, out_dim = n_classes)\n",
    "\n",
    "\n",
    "    def forward(self, img):\n",
    "        \"\"\" [input]\n",
    "            - img (torch.Tensor) : 画像データ\n",
    "                - img.shape = torch.Size([batch_size, channels, image_height, image_width])\n",
    "        \"\"\"\n",
    "\n",
    "        x = img\n",
    "\n",
    "        # 1. パッチに分割\n",
    "        # x.shape : [batch_size, channels, image_height, image_width] -> [batch_size, n_patches, channels * (patch_size ** 2)]\n",
    "        x = self.patching(x)\n",
    "\n",
    "        # 2. 各パッチをベクトルに変換\n",
    "        # x.shape : [batch_size, n_patches, channels * (patch_size ** 2)] -> [batch_size, n_patches, dim]\n",
    "        x = self.linear_projection_of_flattened_patches(x)\n",
    "\n",
    "        # 3. [class] トークン付加 + 位置エンコーディング \n",
    "        # x.shape : [batch_size, n_patches, dim] -> [batch_size, n_patches + 1, dim]\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # 4. Transformer Encoder\n",
    "        # x.shape : No Change\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # 5. 出力の0番目のベクトルを MLP Head で処理\n",
    "        # x.shape : [batch_size, n_patches + 1, dim] -> [batch_size, dim] -> [batch_size, n_classes]\n",
    "        x = x[:, 0]\n",
    "        x = self.mlp_head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. パッチに分割\n",
    "<div align=\"center\">\n",
    "<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F585587%2F272cb2b1-9c9c-1c28-cf4c-d58927226109.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=394c70144fdeafec0d1a45d107ce5faf\" width=\"60%\">\n",
    "</div>\n",
    "\n",
    "- 一枚の画像をパッチに分割する。画像を９個に切り分けて、左上から横に並べていくだけ\n",
    "- 元の画像は[C,H,W]の三次元配列であったが、切り分けた後はC*P^2の一次元配列になっているので、xのサイズは<br>\n",
    "    [B,C,H,W]→[B,N,C*P^2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patching(nn.Module):\n",
    "    def __init__(self, patch_size):\n",
    "        \"\"\" [input]\n",
    "            - patch_size (int) : パッチの縦の長さ（=横の長さ）\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = Rearrange(\"b c (h ph) (w pw) -> b (h w) (ph pw c)\", ph = patch_size, pw = patch_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" [input]\n",
    "            - x (torch.Tensor) : 画像データ\n",
    "                - x.shape = torch.Size([batch_size, channels, image_height, image_width])\n",
    "        \"\"\"\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.各パッチをベクトルに変換\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F585587%2F993cc372-2e26-817a-21d8-55911af9403a.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=0300afbe0e313c4d2d5f123b5e7414b8\" width=\"60%\">\n",
    "</div>\n",
    "\n",
    "- 各パッチのベクトルを別サイズのベクトルに変換する。各パッチのベクトルの長さ(patch_dim)はC*P^2になる\n",
    "- nn.Linearの部分の行列も学習可能なパラメーター"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearProjection(nn.Module):\n",
    "    def __init__(self, patch_dim, dim):\n",
    "        \"\"\" [input]\n",
    "            - patch_dim (int) : 一枚あたりのパッチの次元（= channels * (patch_size ** 2)）\n",
    "            - dim (int) : パッチが変換されたベクトルの次元 \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(patch_dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" [input]\n",
    "            - x (torch.Tensor) \n",
    "                - x.shape = torch.Size([batch_size, n_patches, patch_dim])\n",
    "        \"\"\"\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.[class]トークン付与、位置エンコーディング\n",
    "<div align=\"center\">\n",
    "<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F585587%2F134cffbb-2684-a452-8865-72d7a2dc3fba.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=2866cff924c0a6dfa5e8b46f667a89cf\" width=\"60%\">\n",
    "</div>\n",
    "「2. 各パッチをベクトルに変換」によって作られた \n",
    "（コード中「n_patches」）個のパッチのベクトル達の先頭に [class] トークンを付加する。<br>\n",
    "これは学習可能なパラメータで、Transformer Encoder によって処理された後の [class] トークンに対応する部分（正確にはそれを nn.Linear(dim, n_classes) で処理したもの）が、予測結果を返してくれる。\n",
    "\n",
    "この時点で、x のサイズは [B,N+1,D] となる。（[class] トークンの分）。\n",
    "\n",
    "その後、位置エンコーディングを行う。後ほど説明する Transformer Encoder では、入力トークンの位置情報を把握することができないため、位置情報をあらかじめ付加する必要あり。<br>\n",
    "実装としては、(N+1)*Dの行列 を加算します。これは、学習可能なパラメータ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, dim, n_patches):\n",
    "        \"\"\" [input]\n",
    "            - dim (int) : パッチが変換されたベクトルの次元\n",
    "            - n_patches (int) : パッチの枚数\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # class token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "\n",
    "        # position embedding\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, n_patches + 1, dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"[input]\n",
    "            - x (torch.Tensor)\n",
    "                - x.shape = torch.Size([batch_size, n_patches, dim])\n",
    "        \"\"\"\n",
    "        # バッチサイズを抽出\n",
    "        batch_size, _, __ = x.shape\n",
    "\n",
    "        # [class] トークン付加\n",
    "        # x.shape : [batch_size, n_patches, patch_dim] -> [batch_size, n_patches + 1, patch_dim]\n",
    "        cls_tokens = repeat(self.cls_token, \"1 1 d -> b 1 d\", b = batch_size)\n",
    "        x = torch.concat([cls_tokens, x], dim = 1)\n",
    "\n",
    "        # 位置エンコーディング加算\n",
    "        x += self.pos_embedding\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Transformer Encoder\n",
    "<div align=\"center\">\n",
    "<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F585587%2Fe8ecf326-2b6c-9f38-8030-7d5b38017c0b.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=bcb0dee645ba5221eb27085fddb38459\" width=\"20%\">\n",
    "</div>\n",
    "\n",
    "Transformer Encoderの構成要素は四つに分かれている\n",
    "- 残差接続\n",
    "- Layer Normalization(Norm)\n",
    "- Multi-Head Self-Atteintion\n",
    "- Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, dim, n_heads, mlp_dim, depth):\n",
    "        \"\"\" [input]\n",
    "            - dim (int) : 各パッチのベクトルが変換されたベクトルの長さ（参考[1] (1)式 D）\n",
    "            - depth (int) : Transformer Encoder の層の深さ（参考[1] (2)式 L）\n",
    "            - n_heads (int) : Multi-Head Attention の head の数\n",
    "            - mlp_dim (int) : MLP の隠れ層のノード数\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Layers\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.multi_head_attention = MultiHeadAttention(dim = dim, n_heads = n_heads)\n",
    "        self.mlp = MLP(dim = dim, hidden_dim = mlp_dim)\n",
    "        self.depth = depth\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"[input]\n",
    "            - x (torch.Tensor)\n",
    "                - x.shape = torch.Size([batch_size, n_patches + 1, dim])\n",
    "        \"\"\"\n",
    "        for _ in range(self.depth):\n",
    "            x = self.multi_head_attention(self.norm(x)) + x\n",
    "            x = self.mlp(self.norm(x)) + x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim, n_heads):\n",
    "        \"\"\" [input]\n",
    "            - dim (int) : パッチのベクトルが変換されたベクトルの長さ\n",
    "            - n_heads (int) : heads の数\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim_heads = dim // n_heads\n",
    "\n",
    "        self.W_q = nn.Linear(dim, dim)\n",
    "        self.W_k = nn.Linear(dim, dim)\n",
    "        self.W_v = nn.Linear(dim, dim)\n",
    "\n",
    "        self.split_into_heads = Rearrange(\"b n (h d) -> b h n d\", h = self.n_heads)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "\n",
    "        self.concat = Rearrange(\"b h n d -> b n (h d)\", h = self.n_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"[input]\n",
    "            - x (torch.Tensor)\n",
    "                - x.shape = torch.Size([batch_size, n_patches + 1, dim])\n",
    "        \"\"\"\n",
    "        q = self.W_q(x)\n",
    "        k = self.W_k(x)\n",
    "        v = self.W_v(x)\n",
    "\n",
    "        q = self.split_into_heads(q)\n",
    "        k = self.split_into_heads(k)\n",
    "        v = self.split_into_heads(v)\n",
    "\n",
    "        # Logit[i] = Q[i] * tK[i] / sqrt(D) (i = 1, ... , n_heads)\n",
    "        # AttentionWeight[i] = Softmax(Logit[i]) (i = 1, ... , n_heads)\n",
    "        logit = torch.matmul(q, k.transpose(-1, -2)) * (self.dim_heads ** -0.5)\n",
    "        attention_weight = self.softmax(logit)\n",
    "\n",
    "        # Head[i] = AttentionWeight[i] * V[i] (i = 1, ... , n_heads)\n",
    "        # Output = concat[Head[1], ... , Head[n_heads]]\n",
    "        output = torch.matmul(attention_weight, v)\n",
    "        output = self.concat(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        \"\"\" [input]\n",
    "            - dim (int) : パッチのベクトルが変換されたベクトルの長さ\n",
    "            - hidden_dim (int) : 隠れ層のノード数\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"[input]\n",
    "            - x (torch.Tensor)\n",
    "                - x.shape = torch.Size([batch_size, n_patches + 1, dim])\n",
    "        \"\"\"\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.MLP Head\n",
    "<div align=\"center\">\n",
    "<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F585587%2Ff461422f-4f4e-7ab2-bad1-017d3100abf3.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=ea2944078c2fd27bb3e2076635883139\" width=\"50%\">\n",
    "</div>\n",
    "\n",
    "Transformer Encoder で処理された後の [class] トークンに対応する部分を MLP Head で処理。具体的には、最初に Layer Norm で処理し、その後、クラスの数の長さのベクトルに線形で変換。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, out_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = ViT(\n",
    "    image_size=32,\n",
    "    patch_size=4,\n",
    "    n_classes=10,\n",
    "    dim=256,\n",
    "    depth=3,\n",
    "    n_heads=4,\n",
    "    mlp_dim = 256\n",
    ").to(device)\n",
    "\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1, Train [Loss: 2.069, Accuracy: 0.235], Valid [Loss: 1.945, Accuracy: 0.300]]\n",
      "EPOCH: 2, Train [Loss: 1.785, Accuracy: 0.360], Valid [Loss: 1.750, Accuracy: 0.374]]\n",
      "EPOCH: 3, Train [Loss: 1.683, Accuracy: 0.396], Valid [Loss: 1.660, Accuracy: 0.407]]\n",
      "EPOCH: 4, Train [Loss: 1.596, Accuracy: 0.430], Valid [Loss: 1.560, Accuracy: 0.443]]\n",
      "EPOCH: 5, Train [Loss: 1.531, Accuracy: 0.451], Valid [Loss: 1.538, Accuracy: 0.450]]\n",
      "EPOCH: 6, Train [Loss: 1.475, Accuracy: 0.469], Valid [Loss: 1.442, Accuracy: 0.489]]\n",
      "EPOCH: 7, Train [Loss: 1.422, Accuracy: 0.489], Valid [Loss: 1.436, Accuracy: 0.483]]\n",
      "EPOCH: 8, Train [Loss: 1.365, Accuracy: 0.508], Valid [Loss: 1.390, Accuracy: 0.506]]\n",
      "EPOCH: 9, Train [Loss: 1.314, Accuracy: 0.527], Valid [Loss: 1.362, Accuracy: 0.511]]\n",
      "EPOCH: 10, Train [Loss: 1.273, Accuracy: 0.540], Valid [Loss: 1.326, Accuracy: 0.526]]\n",
      "EPOCH: 11, Train [Loss: 1.232, Accuracy: 0.557], Valid [Loss: 1.291, Accuracy: 0.543]]\n",
      "EPOCH: 12, Train [Loss: 1.192, Accuracy: 0.571], Valid [Loss: 1.266, Accuracy: 0.554]]\n",
      "EPOCH: 13, Train [Loss: 1.150, Accuracy: 0.586], Valid [Loss: 1.257, Accuracy: 0.550]]\n",
      "EPOCH: 14, Train [Loss: 1.112, Accuracy: 0.600], Valid [Loss: 1.260, Accuracy: 0.560]]\n",
      "EPOCH: 15, Train [Loss: 1.072, Accuracy: 0.614], Valid [Loss: 1.282, Accuracy: 0.554]]\n",
      "EPOCH: 16, Train [Loss: 1.040, Accuracy: 0.623], Valid [Loss: 1.243, Accuracy: 0.561]]\n",
      "EPOCH: 17, Train [Loss: 1.004, Accuracy: 0.640], Valid [Loss: 1.259, Accuracy: 0.563]]\n",
      "EPOCH: 18, Train [Loss: 0.965, Accuracy: 0.653], Valid [Loss: 1.273, Accuracy: 0.559]]\n",
      "EPOCH: 19, Train [Loss: 0.932, Accuracy: 0.666], Valid [Loss: 1.271, Accuracy: 0.565]]\n",
      "EPOCH: 20, Train [Loss: 0.893, Accuracy: 0.676], Valid [Loss: 1.274, Accuracy: 0.562]]\n",
      "EPOCH: 21, Train [Loss: 0.857, Accuracy: 0.691], Valid [Loss: 1.297, Accuracy: 0.568]]\n",
      "EPOCH: 22, Train [Loss: 0.817, Accuracy: 0.705], Valid [Loss: 1.383, Accuracy: 0.553]]\n",
      "EPOCH: 23, Train [Loss: 0.785, Accuracy: 0.715], Valid [Loss: 1.350, Accuracy: 0.561]]\n",
      "EPOCH: 24, Train [Loss: 0.746, Accuracy: 0.728], Valid [Loss: 1.424, Accuracy: 0.550]]\n",
      "EPOCH: 25, Train [Loss: 0.705, Accuracy: 0.744], Valid [Loss: 1.420, Accuracy: 0.558]]\n",
      "EPOCH: 26, Train [Loss: 0.670, Accuracy: 0.757], Valid [Loss: 1.444, Accuracy: 0.559]]\n",
      "EPOCH: 27, Train [Loss: 0.627, Accuracy: 0.773], Valid [Loss: 1.492, Accuracy: 0.558]]\n",
      "EPOCH: 28, Train [Loss: 0.610, Accuracy: 0.779], Valid [Loss: 1.566, Accuracy: 0.541]]\n",
      "EPOCH: 29, Train [Loss: 0.573, Accuracy: 0.792], Valid [Loss: 1.620, Accuracy: 0.546]]\n",
      "EPOCH: 30, Train [Loss: 0.541, Accuracy: 0.804], Valid [Loss: 1.594, Accuracy: 0.550]]\n",
      "EPOCH: 31, Train [Loss: 0.499, Accuracy: 0.822], Valid [Loss: 1.655, Accuracy: 0.549]]\n",
      "EPOCH: 32, Train [Loss: 0.466, Accuracy: 0.831], Valid [Loss: 1.688, Accuracy: 0.545]]\n",
      "EPOCH: 33, Train [Loss: 0.431, Accuracy: 0.846], Valid [Loss: 1.748, Accuracy: 0.555]]\n",
      "EPOCH: 34, Train [Loss: 0.413, Accuracy: 0.853], Valid [Loss: 1.866, Accuracy: 0.548]]\n",
      "EPOCH: 35, Train [Loss: 0.397, Accuracy: 0.854], Valid [Loss: 1.818, Accuracy: 0.548]]\n",
      "EPOCH: 36, Train [Loss: 0.359, Accuracy: 0.870], Valid [Loss: 1.934, Accuracy: 0.547]]\n",
      "EPOCH: 37, Train [Loss: 0.325, Accuracy: 0.882], Valid [Loss: 1.979, Accuracy: 0.551]]\n",
      "EPOCH: 38, Train [Loss: 0.306, Accuracy: 0.889], Valid [Loss: 2.043, Accuracy: 0.546]]\n",
      "EPOCH: 39, Train [Loss: 0.284, Accuracy: 0.898], Valid [Loss: 2.045, Accuracy: 0.546]]\n",
      "EPOCH: 40, Train [Loss: 0.276, Accuracy: 0.899], Valid [Loss: 2.137, Accuracy: 0.544]]\n",
      "EPOCH: 41, Train [Loss: 0.255, Accuracy: 0.909], Valid [Loss: 2.250, Accuracy: 0.538]]\n",
      "EPOCH: 42, Train [Loss: 0.241, Accuracy: 0.914], Valid [Loss: 2.242, Accuracy: 0.541]]\n",
      "EPOCH: 43, Train [Loss: 0.206, Accuracy: 0.926], Valid [Loss: 2.323, Accuracy: 0.547]]\n",
      "EPOCH: 44, Train [Loss: 0.183, Accuracy: 0.934], Valid [Loss: 2.390, Accuracy: 0.548]]\n",
      "EPOCH: 45, Train [Loss: 0.179, Accuracy: 0.937], Valid [Loss: 2.452, Accuracy: 0.548]]\n",
      "EPOCH: 46, Train [Loss: 0.150, Accuracy: 0.948], Valid [Loss: 2.604, Accuracy: 0.538]]\n",
      "EPOCH: 47, Train [Loss: 0.182, Accuracy: 0.935], Valid [Loss: 2.580, Accuracy: 0.537]]\n",
      "EPOCH: 48, Train [Loss: 0.174, Accuracy: 0.938], Valid [Loss: 2.563, Accuracy: 0.532]]\n",
      "EPOCH: 49, Train [Loss: 0.148, Accuracy: 0.947], Valid [Loss: 2.604, Accuracy: 0.548]]\n",
      "EPOCH: 50, Train [Loss: 0.128, Accuracy: 0.954], Valid [Loss: 2.631, Accuracy: 0.554]]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(config['n_epochs']):\n",
    "    losses_train = []\n",
    "    losses_valid = []\n",
    "\n",
    "    model.train()\n",
    "    n_train = 0\n",
    "    acc_train = 0\n",
    "    for x, t in dataloader_train:\n",
    "        n_train += t.size()[0]\n",
    "\n",
    "        model.zero_grad()  # 勾配の初期化\n",
    "\n",
    "        x = x.to(device)  # テンソルをGPUに移動\n",
    "        t = t.to(device)\n",
    "\n",
    "        y = model.forward(x)  # 順伝播\n",
    "\n",
    "\n",
    "        loss = loss_function(y, t)  # 誤差(クロスエントロピー誤差関数)の計算\n",
    "\n",
    "        loss.backward()  # 誤差の逆伝播\n",
    "\n",
    "        optimizer.step()  # パラメータの更新\n",
    "\n",
    "        pred = y.argmax(1)  # 最大値を取るラベルを予測ラベルとする\n",
    "\n",
    "        acc_train += (pred == t).float().sum().item()\n",
    "        losses_train.append(loss.tolist())\n",
    "\n",
    "    model.eval()\n",
    "    n_val = 0\n",
    "    acc_val = 0\n",
    "    for x, t in dataloader_valid:\n",
    "        n_val += t.size()[0]\n",
    "\n",
    "        x = x.to(device)  # テンソルをGPUに移動\n",
    "        t = t.to(device)\n",
    "\n",
    "        y = model.forward(x)  # 順伝播\n",
    "\n",
    "        loss = loss_function(y, t)  # 誤差(クロスエントロピー誤差関数)の計算\n",
    "\n",
    "        pred = y.argmax(1)  # 最大値を取るラベルを予測ラベルとする\n",
    "\n",
    "        acc_val += (pred == t).float().sum().item()\n",
    "        losses_valid.append(loss.tolist())\n",
    "\n",
    "    print('EPOCH: {}, Train [Loss: {:.3f}, Accuracy: {:.3f}], Valid [Loss: {:.3f}, Accuracy: {:.3f}]]'.format(\n",
    "        epoch+1,\n",
    "        np.mean(losses_train),\n",
    "        acc_train/n_train,\n",
    "        np.mean(losses_valid),\n",
    "        acc_val/n_val,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Accuracy of the network on the 10000 test images: 54 %\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor() )\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "# 勾配を記憶せず（学習せずに）に計算を行う\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device),data[1].to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
