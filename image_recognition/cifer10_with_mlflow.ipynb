{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import mlflow\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1  # 可視化の際に扱いやすくするために1とする．\n",
    "\n",
    "# CIFER10の精度を競うことにする\n",
    "# transforms.ToTensr: 入力データ（np.ndarrayなど）をPyTorchのテンソルに変換する．PyTorchで画像を扱うときは(バッチサイズ，チャネル数，高さ，幅）になるため注意．\n",
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('../data/cifar10', train=True, download=True, transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Out層の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(nn.Module):\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        super().__init__()\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            self.mask = torch.rand(*x.size()) > self.dropout_ratio\n",
    "            return x * self.mask.to(x.device)\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch_normで画像の正規化をする。これを最終的な層に組み込むことになる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, shape, epsilon=np.float32(1e-5)):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.tensor(np.ones(shape, dtype='float32')))\n",
    "        self.beta = nn.Parameter(torch.tensor(np.zeros(shape, dtype='float32')))\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = torch.mean(x, (0, 2, 3), keepdim=True)  \n",
    "        std = torch.std(x, (0, 2, 3), keepdim=True)  \n",
    "        x_normalized = (x - mean) / (std**2 + self.epsilon)**0.5  \n",
    "        return self.gamma * x_normalized + self.beta  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    def __init__(self, filter_shape, function=lambda x: x, stride=(1, 1), padding=0):\n",
    "        super().__init__()\n",
    "        # Heの初期化\n",
    "        # filter_shape: (出力チャンネル数)x(入力チャンネル数)x(縦の次元数)x(横の次元数)\n",
    "        fan_in = filter_shape[1] * filter_shape[2] * filter_shape[3]\n",
    "        fan_out = filter_shape[0] * filter_shape[2] * filter_shape[3]\n",
    "\n",
    "        self.W = nn.Parameter(torch.tensor(rng.normal(\n",
    "                        0,\n",
    "                        np.sqrt(2/fan_in),\n",
    "                        size=filter_shape\n",
    "                    ).astype('float32')))\n",
    "\n",
    "        # バイアスはフィルタごとなので, 出力フィルタ数と同じ次元数\n",
    "        self.b = nn.Parameter(torch.tensor(np.zeros((filter_shape[0]), dtype='float32')))\n",
    "\n",
    "        self.function = function  \n",
    "        self.stride = stride  \n",
    "        self.padding = padding \n",
    "\n",
    "    def forward(self, x):\n",
    "        u = F.conv2d(x, self.W, bias=self.b, stride=self.stride, padding=self.padding)\n",
    "        return self.function(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling(nn.Module):\n",
    "    def __init__(self, ksize=(2, 2), stride=(2, 2), padding=0):\n",
    "        super().__init__()\n",
    "        self.ksize = ksize  # カーネルサイズ\n",
    "        self.stride = stride  # ストライド幅\n",
    "        self.padding = padding  # パディング\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.avg_pool2d(x, kernel_size=self.ksize, stride=self.stride, padding=self.padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x.view(x.size()[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x.sizeで返ってくるのは、(Batch_size,Channel,Height,Width)のtuple型が返ってくるが、それの０番目を指定することでBatch_sizeが縦に並ぶベクトルのデータが出来上がる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(nn.Module):\n",
    "    def __init__(self,in_dim,out_dim,function=lambda x:x):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.w = nn.Parameter((torch.tensor(rng.normal(0,np.sqrt(2/in_dim),size=(in_dim, out_dim)).astype('float32'))))\n",
    "\n",
    "        self.b = nn.Parameter(torch.tensor(np.zeros([out_dim]).astype('float32')))\n",
    "        self.function = function\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.function(torch.matmul(x,self.w)+self.b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(nn.Module):\n",
    "    def __init__(self, function=lambda x: x):\n",
    "        super().__init__()\n",
    "        self.function = function\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.function(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_log(x):\n",
    "    return torch.log(torch.clamp(x, min=1e-10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "configでハイパーパラメータを管理する。最後のパラメータはピリオドになることの注意する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    'batch_size': 100,\n",
    "    'epochs': 50,\n",
    "    'seed': 42,\n",
    "    'lr': 0.01,\n",
    "    'num_layers' : 3,\n",
    "    'n_resch' : 64.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nv_netで実際のモデルを定義する。\n",
    "画像のチャネル数はハイパーパラメータ扱いになる。これは自分で決めることができるし最適化されたパラメータがあるはず。\n",
    "最初の畳み込み層のConvは引数が画像の (縦横のサイズ、チャネル数、カーネルの大きさ) になっていて、畳み込み計算のときにパディングをするので縦横の画像サイズは２だけ減る。\n",
    "Batch_normalizationの引数は (チャネル数、縦のサイズ、横のサイズ) の順番\n",
    "Pooling層は２×２なので純粋に画像のサイズが半分になる。\n",
    "これを２×２のサイズまで持ってきて最後に平滑化させてDNNに入れて学習させる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conv_net = nn.Sequential(\n",
    "    Conv((32, 3, 3, 3)),        # 画像の大きさ：32x32x3 -> 30x30x32  (入出力の画像サイズ）\n",
    "    BatchNorm((32, 30, 30)),\n",
    "    Activation(F.relu),\n",
    "    Pooling((2, 2)),            # 30x30x32 -> 15x15x32  (入出力の画像サイズ）\n",
    "    Conv((64, 32, 3, 3)),       # 15x15x32 -> 13x13x64  (入出力の画像サイズ）\n",
    "    BatchNorm((64, 13, 13)),\n",
    "    Activation(F.relu),\n",
    "    Pooling((2, 2)),            # 13x13x64 -> 6x6x64  (入出力の画像サイズ）\n",
    "    Conv((128, 64, 3, 3)),      # 6x6x64 -> 4x4x128  (入出力の画像サイズ）\n",
    "    BatchNorm((128, 4, 4)),\n",
    "    Activation(F.relu),\n",
    "    Pooling((2, 2)),            # 4x4x128 -> 2x2x128 (入出力の画像サイズ）\n",
    "    Flatten(),\n",
    "    Dense(2*2*128, 256, F.relu),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, 10)\n",
    ")\n",
    "device='mps'\n",
    "conv_net.to(device)\n",
    "optimizer = optim.Adam(conv_net.parameters(), lr=config['lr'])\n",
    "batch_size=config['batch_size']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data number:40000, Valid data number: 10000\n"
     ]
    }
   ],
   "source": [
    "trainval_dataset = datasets.CIFAR10('../data/cifar10', train=True, transform=transforms.ToTensor())\n",
    "\n",
    "# 前処理を定義\n",
    "transform = transforms.Compose([transforms.RandomRotation(degrees=(-180, 180)), transforms.ToTensor()])\n",
    "\n",
    "trainval_dataset = datasets.CIFAR10('../data/cifar10', train=True, transform=transform)\n",
    "\n",
    "# trainとvalidに分割\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(trainval_dataset, [len(trainval_dataset)-10000, 10000],generator=torch.Generator().manual_seed(config['seed']))\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataloader_valid = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"Train data number:{}, Valid data number: {}\".format(len(train_dataset), len(val_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/maoya23/deep/machine-learning/image_recognition/cifer10_with_mlflow.ipynb セル 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maoya23/deep/machine-learning/image_recognition/cifer10_with_mlflow.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m n_train \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m  \u001b[39m# 訓練データ数\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maoya23/deep/machine-learning/image_recognition/cifer10_with_mlflow.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m acc_train \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m  \u001b[39m# 訓練データに対する精度\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/maoya23/deep/machine-learning/image_recognition/cifer10_with_mlflow.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m x, t \u001b[39min\u001b[39;00m dataloader_train:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maoya23/deep/machine-learning/image_recognition/cifer10_with_mlflow.ipynb#X24sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     n_train \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m t\u001b[39m.\u001b[39msize()[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maoya23/deep/machine-learning/image_recognition/cifer10_with_mlflow.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     conv_net\u001b[39m.\u001b[39mzero_grad()  \u001b[39m# 勾配の初期化\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/deep/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/deep/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/deep/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/deep/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/deep/lib/python3.10/site-packages/torch/utils/data/dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[0;32m~/anaconda3/envs/deep/lib/python3.10/site-packages/torchvision/datasets/cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    115\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img)\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/anaconda3/envs/deep/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/deep/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/deep/lib/python3.10/site-packages/torchvision/transforms/transforms.py:1379\u001b[0m, in \u001b[0;36mRandomRotation.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1376\u001b[0m         fill \u001b[39m=\u001b[39m [\u001b[39mfloat\u001b[39m(f) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m fill]\n\u001b[1;32m   1377\u001b[0m angle \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_params(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdegrees)\n\u001b[0;32m-> 1379\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mrotate(img, angle, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexpand, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcenter, fill)\n",
      "File \u001b[0;32m~/anaconda3/envs/deep/lib/python3.10/site-packages/torchvision/transforms/functional.py:1129\u001b[0m, in \u001b[0;36mrotate\u001b[0;34m(img, angle, interpolation, expand, center, fill)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m   1128\u001b[0m     pil_interpolation \u001b[39m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m-> 1129\u001b[0m     \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39;49mrotate(img, angle\u001b[39m=\u001b[39;49mangle, interpolation\u001b[39m=\u001b[39;49mpil_interpolation, expand\u001b[39m=\u001b[39;49mexpand, center\u001b[39m=\u001b[39;49mcenter, fill\u001b[39m=\u001b[39;49mfill)\n\u001b[1;32m   1131\u001b[0m center_f \u001b[39m=\u001b[39m [\u001b[39m0.0\u001b[39m, \u001b[39m0.0\u001b[39m]\n\u001b[1;32m   1132\u001b[0m \u001b[39mif\u001b[39;00m center \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/deep/lib/python3.10/site-packages/torchvision/transforms/_functional_pil.py:312\u001b[0m, in \u001b[0;36mrotate\u001b[0;34m(img, angle, interpolation, expand, center, fill)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimg should be PIL Image. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(img)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    311\u001b[0m opts \u001b[39m=\u001b[39m _parse_fill(fill, img)\n\u001b[0;32m--> 312\u001b[0m \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mrotate(angle, interpolation, expand, center, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mopts)\n",
      "File \u001b[0;32m~/anaconda3/envs/deep/lib/python3.10/site-packages/PIL/Image.py:2342\u001b[0m, in \u001b[0;36mImage.rotate\u001b[0;34m(self, angle, resample, expand, center, translate, fillcolor)\u001b[0m\n\u001b[1;32m   2339\u001b[0m     matrix[\u001b[39m2\u001b[39m], matrix[\u001b[39m5\u001b[39m] \u001b[39m=\u001b[39m transform(\u001b[39m-\u001b[39m(nw \u001b[39m-\u001b[39m w) \u001b[39m/\u001b[39m \u001b[39m2.0\u001b[39m, \u001b[39m-\u001b[39m(nh \u001b[39m-\u001b[39m h) \u001b[39m/\u001b[39m \u001b[39m2.0\u001b[39m, matrix)\n\u001b[1;32m   2340\u001b[0m     w, h \u001b[39m=\u001b[39m nw, nh\n\u001b[0;32m-> 2342\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(\n\u001b[1;32m   2343\u001b[0m     (w, h), Transform\u001b[39m.\u001b[39;49mAFFINE, matrix, resample, fillcolor\u001b[39m=\u001b[39;49mfillcolor\n\u001b[1;32m   2344\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/deep/lib/python3.10/site-packages/PIL/Image.py:2713\u001b[0m, in \u001b[0;36mImage.transform\u001b[0;34m(self, size, method, data, resample, fill, fillcolor)\u001b[0m\n\u001b[1;32m   2709\u001b[0m         im\u001b[39m.\u001b[39m__transformer(\n\u001b[1;32m   2710\u001b[0m             box, \u001b[39mself\u001b[39m, Transform\u001b[39m.\u001b[39mQUAD, quad, resample, fillcolor \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2711\u001b[0m         )\n\u001b[1;32m   2712\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2713\u001b[0m     im\u001b[39m.\u001b[39;49m__transformer(\n\u001b[1;32m   2714\u001b[0m         (\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m) \u001b[39m+\u001b[39;49m size, \u001b[39mself\u001b[39;49m, method, data, resample, fillcolor \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m   2715\u001b[0m     )\n\u001b[1;32m   2717\u001b[0m \u001b[39mreturn\u001b[39;00m im\n",
      "File \u001b[0;32m~/anaconda3/envs/deep/lib/python3.10/site-packages/PIL/Image.py:2796\u001b[0m, in \u001b[0;36mImage.__transformer\u001b[0;34m(self, box, image, method, data, resample, fill)\u001b[0m\n\u001b[1;32m   2793\u001b[0m \u001b[39mif\u001b[39;00m image\u001b[39m.\u001b[39mmode \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mP\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   2794\u001b[0m     resample \u001b[39m=\u001b[39m Resampling\u001b[39m.\u001b[39mNEAREST\n\u001b[0;32m-> 2796\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mim\u001b[39m.\u001b[39;49mtransform2(box, image\u001b[39m.\u001b[39;49mim, method, data, resample, fill)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy_train=[]\n",
    "accuracy_valid=[]\n",
    "cost_train=[]\n",
    "cost_valid=[]#このリストをforの中にいれてしまうと新しいリストが毎回作られてしまう。必ず外に出すこと\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    losses_train = []  # 訓練誤差を格納しておくリスト\n",
    "    losses_valid = []  # 検証データの誤差を格納しておくリスト\n",
    "\n",
    "    conv_net.train()  # 訓練モードにする\n",
    "    n_train = 0  # 訓練データ数\n",
    "    acc_train = 0  # 訓練データに対する精度\n",
    "    for x, t in dataloader_train:\n",
    "        n_train += t.size()[0]\n",
    "\n",
    "        conv_net.zero_grad()  # 勾配の初期化\n",
    "\n",
    "        x = x.to(device)  # テンソルをGPUに移動\n",
    "\n",
    "        t_hot = torch.eye(10)[t]  # 正解ラベルをone-hot vector化\n",
    "\n",
    "        t = t.to(device)\n",
    "        t_hot = t_hot.to(device)  # 正解ラベルとone-hot vectorをそれぞれGPUに移動\n",
    "\n",
    "        y = conv_net.forward(x)  # 順伝播\n",
    "\n",
    "        loss = -(t_hot*torch.log_softmax(y, dim=-1)).sum(axis=1).mean()  # 誤差(クロスエントロピー誤差関数)の計算\n",
    "\n",
    "        loss.backward()  # 誤差の逆伝播\n",
    "\n",
    "        optimizer.step()  # パラメータの更新\n",
    "\n",
    "        pred = y.argmax(1)  # 最大値を取るラベルを予測ラベルとする\n",
    "\n",
    "        acc_train += (pred == t).float().sum().item()\n",
    "        losses_train.append(loss.tolist())\n",
    "\n",
    "    accuracy_train.append(acc_train/n_train)\n",
    "    cost_train.append(np.mean(losses_train))\n",
    "\n",
    "    conv_net.eval()  # 評価モードにする\n",
    "    n_val = 0\n",
    "    acc_val = 0\n",
    "    for x, t in dataloader_valid:\n",
    "        n_val += t.size()[0]\n",
    "\n",
    "        x = x.to(device)  # テンソルをGPUに移動\n",
    "\n",
    "        t_hot = torch.eye(10)[t]  # 正解ラベルをone-hot vector化\n",
    "\n",
    "        t = t.to(device)\n",
    "        t_hot = t_hot.to(device)  # 正解ラベルとone-hot vectorをそれぞれGPUに移動\n",
    "\n",
    "        y = conv_net.forward(x)  # 順伝播\n",
    "\n",
    "        loss = -(t_hot*torch.log_softmax(y, dim=-1)).sum(axis=1).mean()  # 誤差(クロスエントロピー誤差関数)の計算\n",
    "\n",
    "        pred = y.argmax(1)  # 最大値を取るラベルを予測ラベルとする\n",
    "\n",
    "        acc_val += (pred == t).float().sum().item()\n",
    "        losses_valid.append(loss.tolist())\n",
    "\n",
    "    accuracy_valid.append(acc_val/n_val)\n",
    "    cost_valid.append(np.mean(losses_valid))\n",
    "\n",
    "    print('EPOCH: {}, Train [Loss: {:.3f}, Accuracy: {:.3f}], Valid [Loss: {:.3f}, Accuracy: {:.3f}]'.format(\n",
    "        epoch+1,\n",
    "        np.mean(losses_train),\n",
    "        acc_train/n_train,\n",
    "        np.mean(losses_valid),\n",
    "        acc_val/n_val\n",
    "    ))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x=np.arange(1,config['epochs']+1,1)\n",
    "y1=accuracy_train\n",
    "y2=cost_train\n",
    "y3 = accuracy_valid\n",
    "y4 = cost_valid\n",
    "c1,c2= 'blue', 'orange'\n",
    "l1,l2,l3,l4 = 'accuracy_train', 'cost_train','accuracy_valid','cost_valid'\n",
    "xl1, xl2= 'epochs', 'epochs'\n",
    "yl1, yl2= 'accuracy', 'cost'\n",
    "fig = plt.figure(figsize = (20,6))\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax1.plot(x, y1, color=c1, label=l1)\n",
    "ax1.plot(x,y3,color=c2,label=l3)\n",
    "ax2.plot(x, y2, color=c1, label=l2)\n",
    "ax2.plot(x,y4,color=c2,label=l4)\n",
    "ax1.set_xlabel(xl1)\n",
    "ax2.set_xlabel(xl2)\n",
    "ax1.set_ylabel(yl1)\n",
    "ax2.set_ylabel(yl2)\n",
    "ax1.legend(loc = 'upper right') \n",
    "ax2.legend(loc = 'upper right') \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cosine sheduler で学習率を最初は大きくして学習が進むにつれて学習率をだんだん小さくしていくように設定する(matureな神経細胞を表現している)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineScheduler:\n",
    "    def __init__(self, epochs, lr,warmup_length=5):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        epochs : int\n",
    "            学習のエポック数．\n",
    "        lr : float\n",
    "            学習率．\n",
    "        warmup_length : int\n",
    "            warmupを適用するエポック数\n",
    "        \"\"\"\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.warmup = warmup_length\n",
    "\n",
    "    def __call__(self, epoch):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        epoch : int\n",
    "            現在のエポック数．\n",
    "        \"\"\"\n",
    "        progress = (epoch - self.warmup +1e-6) / (self.epochs - self.warmup +1e-6)\n",
    "        progress = np.clip(progress, 0.0, 1.0)\n",
    "        lr = self.lr * 0.5 * (1. + np.cos(np.pi * progress))\n",
    "\n",
    "        if self.warmup:\n",
    "            lr = lr * min(1., (epoch+1) / self.warmup)\n",
    "\n",
    "        return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_lr(lr,optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"]=lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_tuning={\n",
    "'lr':[0.01,0.001,0.0001],'warmup':[10,20]\n",
    "}\n",
    "\n",
    "config_normal={\n",
    "    'epochs':100,\n",
    "    'batch_size':100\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(lr,warmup):\n",
    "    for epoch in range(config_normal['epochs']):\n",
    "        scheduler=CosineScheduler(epoch,lr,warmup)\n",
    "        new_lr=scheduler(epoch)\n",
    "        set_lr(new_lr,optimizer)\n",
    "\n",
    "        losses_train=[]\n",
    "        losses_valid=[]\n",
    "\n",
    "        conv_net.train()\n",
    "        n_train=0\n",
    "        acc_train=0\n",
    "        for x,t in dataloader_train:\n",
    "            n_train += t.size()[0]\n",
    "            conv_net.zero_grad()\n",
    "            x=x.to(device)\n",
    "            t_hot=torch.eye(10)[t]\n",
    "            t=t.to(device)\n",
    "            t_hot=t_hot.to(device)\n",
    "\n",
    "            y=conv_net.forward(x)\n",
    "            loss=-(t_hot*torch.log_softmax(y,dim=-1)).sum(axis=1).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pred=y.argmax(1)\n",
    "\n",
    "            acc_train+=(pred==t).float().sum().item()#pred==tでブール値を取り出して、.floatで数字に変換してそれのsumを取る.itemでテンソルの値をスカラー値に変換する。\n",
    "            losses_train.append(loss.tolist())  \n",
    "\n",
    "            train_loss=np.mean(losses_train)\n",
    "            train_accuracy=acc_train/n_train\n",
    "\n",
    "\n",
    "        conv_net.eval()  # 評価モードにする\n",
    "        n_val = 0\n",
    "        acc_val = 0\n",
    "        for x, t in dataloader_valid:\n",
    "            n_val += t.size()[0]\n",
    "\n",
    "            x = x.to(device)  # テンソルをGPUに移動\n",
    "\n",
    "            t_hot = torch.eye(10)[t]  # 正解ラベルをone-hot vector化\n",
    "\n",
    "            t = t.to(device)\n",
    "            t_hot = t_hot.to(device)  # 正解ラベルとone-hot vectorをそれぞれGPUに移動\n",
    "\n",
    "            y = conv_net.forward(x)  # 順伝播\n",
    "\n",
    "            loss = -(t_hot*torch.log_softmax(y, dim=-1)).sum(axis=1).mean()  # 誤差(クロスエントロピー誤差関数)の計算\n",
    "\n",
    "            pred = y.argmax(1)  # 最大値を取るラベルを予測ラベルとする\n",
    "\n",
    "            acc_val += (pred == t).float().sum().item()\n",
    "            losses_valid.append(loss.tolist())\n",
    "\n",
    "            valid_loss=np.mean(losses_valid)\n",
    "            valid_accuracy=acc_val/n_val\n",
    "\n",
    "    return train_loss,train_accuracy,valid_loss,valid_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1: lr=0.01, warmup=10, train_loss = 1.153643743544817, train_accuracy = 0.600025,valid_accuracy=0.5988,valid_loss=0.5988\n",
      "model 2: lr=0.01, warmup=20, train_loss = 1.1567876374721526, train_accuracy = 0.597325,valid_accuracy=0.604,valid_loss=0.604\n",
      "model 3: lr=0.001, warmup=10, train_loss = 1.1557541725039482, train_accuracy = 0.596875,valid_accuracy=0.6057,valid_loss=0.6057\n",
      "model 4: lr=0.001, warmup=20, train_loss = 1.1459682239592075, train_accuracy = 0.601075,valid_accuracy=0.6041,valid_loss=0.6041\n",
      "model 5: lr=0.0001, warmup=10, train_loss = 1.150210823714733, train_accuracy = 0.600025,valid_accuracy=0.6055,valid_loss=0.6055\n",
      "model 6: lr=0.0001, warmup=20, train_loss = 1.1514583104848861, train_accuracy = 0.59845,valid_accuracy=0.6064,valid_loss=0.6064\n"
     ]
    }
   ],
   "source": [
    "model_num=0\n",
    "\n",
    "for lr_params in config_tuning['lr']:\n",
    "    for warmup_params in config_tuning['warmup']:\n",
    "            with mlflow.start_run(nested=True):\n",
    "                model_num +=1\n",
    "                train_loss,train_accuracy,valid_loss,valid_accuracy=model(lr_params,warmup_params)\n",
    "\n",
    "                print(f\"model {model_num}: lr={lr_params}, warmup={warmup_params}, train_loss = {train_loss}, train_accuracy = {train_accuracy},valid_accuracy={valid_accuracy},valid_loss={valid_accuracy}\")\n",
    "\n",
    "                mlflow.log_param('lr',lr_params)\n",
    "                mlflow.log_param('warmup',warmup_params)\n",
    "                mlflow.log_metric('train_accuracy',train_accuracy)\n",
    "                mlflow.log_metric('valid_accuracy',valid_accuracy)\n",
    "                mlflow.log_metric('train_loss',train_loss)\n",
    "                mlflow.log_metric('valid_loss',valid_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
