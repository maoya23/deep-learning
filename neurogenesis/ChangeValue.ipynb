{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6a0c13f2f0>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config={\n",
    "    'BatchSize':128,\n",
    "    'seed':42,\n",
    "    'n_epochs' : 200,\n",
    "    'lr' : 0.001\n",
    "}\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data number:40000, Valid data number: 10000\n"
     ]
    }
   ],
   "source": [
    "trainval_dataset = datasets.CIFAR10('./data/cifar10', train=True, transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "# 前処理を定義\n",
    "transform = transforms.Compose([transforms.ToTensor(),])\n",
    "\n",
    "trainval_dataset = datasets.CIFAR10('./data/cifar10', train=True, transform=transform)\n",
    "\n",
    "# trainとvalidに分割\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(trainval_dataset, [len(trainval_dataset)-10000, 10000])\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['BatchSize'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataloader_valid = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['BatchSize'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"Train data number:{}, Valid data number: {}\".format(len(train_dataset), len(val_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'\\n    def update_weight(self):\\n        if self.training:\\n            self.mask = torch.abs(self.layer.weight.data) < self.threshold\\n            self.layer.weight.data[self.mask] = 0.0\\n            self.mask = self.layer.weight.data == 0.0\\n            new_weights = torch.tensor(self.generate_number(self.mask.sum())).to(self.layer.weight.device)\\n            self.layer.weight.data[self.mask] = new_weights\\n\\n\""
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class NeuroGenesis1(nn.Module):\n",
    "    def __init__(self,linear_layer,threshold=0.01):#def __init__で初期化、この時のself.変数は保持される\n",
    "        super(NeuroGenesis1,self).__init__()\n",
    "        self.layer=linear_layer\n",
    "        self.threshold=threshold\n",
    "\n",
    "    def generate_number(self,n): #get numbers above the Threshold from He initialization \n",
    "        numbers=[]\n",
    "        while len(numbers)<n:\n",
    "            randn = math.sqrt(2/self.layer.in_features)*torch.randn(1)\n",
    "            if randn >= self.threshold:\n",
    "                numbers.append(randn.item())\n",
    "        return numbers            \n",
    "\n",
    "    def update_weight(self):\n",
    "        with torch.no_grad():\n",
    "            mask= torch.abs(self.layer.weight.data) < self.threshold\n",
    "            #self.layer.weight.data[under_threshold] = self.generate_number(self.layer.weight.data[under_threshold])\n",
    "            self.layer.weight.data[mask] =torch.tensor(self.generate_number(self.mask.sum())).to(self.layer.weight.device)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            if epoch > 2:#early stopping よろしく、best_scoreがpatience回停滞したら\n",
    "                self.update_weight()\n",
    "                return self.layer(x)\n",
    "            else:\n",
    "                return x\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "\n",
    "''''\n",
    "    def update_weight(self):\n",
    "        if self.training:\n",
    "            self.mask = torch.abs(self.layer.weight.data) < self.threshold\n",
    "            self.layer.weight.data[self.mask] = 0.0\n",
    "            self.mask = self.layer.weight.data == 0.0\n",
    "            new_weights = torch.tensor(self.generate_number(self.mask.sum())).to(self.layer.weight.device)\n",
    "            self.layer.weight.data[self.mask] = new_weights\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeurogenesisModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeurogenesisModel, self).__init__()\n",
    "        self.conv1=nn.Conv2d(3, 32, 3)       # 32x32x3 -> 30x30x32\n",
    "        self.av1=nn.ReLU()\n",
    "        self.pool1=nn.AvgPool2d(2)                  # 30x30x32 -> 15x15x32\n",
    "        self.conv2=nn.Conv2d(32, 64, 3)             # 15x15x32 -> 13x13x64\n",
    "        self.av2=nn.ReLU()\n",
    "        self.pool2=nn.AvgPool2d(2)                  # 13x13x64 -> 6x6x64               # 4x4x128 -> 2x2x128\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.fc1=nn.Linear(6*6*64,256)\n",
    "\n",
    "        self.genesis=NeuroGenesis1(linear_layer=self.fc1)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.fc2=nn.Linear(256, 10)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.conv1(x)\n",
    "        x=self.av1(x)\n",
    "        x=self.pool1(x)\n",
    "        x=self.conv2(x)\n",
    "        x=self.av2(x)\n",
    "        x=self.pool2(x)     \n",
    "        x=self.flatten(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.genesis(x)\n",
    "        x=self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model=NeurogenesisModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_weights(m):  # Heの初期化\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.0)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "device='cuda'\n",
    "model.to(device)\n",
    "optimizer2 = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "loss_function = nn.CrossEntropyLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (128x2304 and 128x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/bdr/deep-learning/neurogenesis/ChangeValue.ipynb セル 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bdr/deep-learning/neurogenesis/ChangeValue.ipynb#Z2104sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device)  \u001b[39m# テンソルをGPUに移動\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bdr/deep-learning/neurogenesis/ChangeValue.ipynb#Z2104sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/bdr/deep-learning/neurogenesis/ChangeValue.ipynb#Z2104sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m y \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(x)  \u001b[39m# 順伝播\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bdr/deep-learning/neurogenesis/ChangeValue.ipynb#Z2104sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_function(y, t)  \u001b[39m# 誤差(クロスエントロピー誤差関数)の計算\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bdr/deep-learning/neurogenesis/ChangeValue.ipynb#Z2104sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()  \u001b[39m# 誤差の逆伝播\u001b[39;00m\n",
      "\u001b[1;32m/home/bdr/deep-learning/neurogenesis/ChangeValue.ipynb セル 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bdr/deep-learning/neurogenesis/ChangeValue.ipynb#Z2104sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bdr/deep-learning/neurogenesis/ChangeValue.ipynb#Z2104sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenesis(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/bdr/deep-learning/neurogenesis/ChangeValue.ipynb#Z2104sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc2(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bdr/deep-learning/neurogenesis/ChangeValue.ipynb#Z2104sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/deep/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/deep/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (128x2304 and 128x10)"
     ]
    }
   ],
   "source": [
    "accuracy_train=[]\n",
    "cost_train=[]\n",
    "accuracy_valid=[]\n",
    "cost_valid=[]\n",
    "\n",
    "for epoch in range(config['n_epochs']):\n",
    "    losses_train = []\n",
    "    losses_valid = []\n",
    "\n",
    "    model.train()\n",
    "    n_train = 0\n",
    "    acc_train = 0\n",
    "    for x, t in dataloader_train:\n",
    "        n_train += t.size()[0]\n",
    "\n",
    "        model.zero_grad()  # 勾配の初期化\n",
    "\n",
    "        x = x.to(device)  # テンソルをGPUに移動\n",
    "        t = t.to(device)\n",
    "        y = model.forward(x)  # 順伝播\n",
    "\n",
    "\n",
    "        loss = loss_function(y, t)  # 誤差(クロスエントロピー誤差関数)の計算\n",
    "\n",
    "        loss.backward()  # 誤差の逆伝播\n",
    "\n",
    "        optimizer2.step()  # パラメータの更新\n",
    "        pred = y.argmax(1)  # 最大値を取るラベルを予測ラベルとする\n",
    "\n",
    "        acc_train += (pred == t).float().sum().item()\n",
    "        losses_train.append(loss.tolist())\n",
    "\n",
    "    accuracy_train.append(acc_train/n_train)\n",
    "    cost_train.append(np.mean(losses_train))\n",
    "\n",
    "    model.eval()\n",
    "    n_val = 0\n",
    "    acc_val = 0\n",
    "    for x, t in dataloader_valid:\n",
    "        n_val += t.size()[0]\n",
    "\n",
    "        x = x.to(device)  # テンソルをGPUに移動\n",
    "        t = t.to(device)\n",
    "\n",
    "        y = model.forward(x)  # 順伝播\n",
    "\n",
    "        loss = loss_function(y, t)  # 誤差(クロスエントロピー誤差関数)の計算\n",
    "\n",
    "        pred = y.argmax(1)  # 最大値を取るラベルを予測ラベルとする\n",
    "\n",
    "        acc_val += (pred == t).float().sum().item()\n",
    "        losses_valid.append(loss.tolist())\n",
    "    accuracy_valid.append(acc_val/n_val)\n",
    "    cost_valid.append(np.mean(losses_valid))\n",
    "\n",
    "    print('EPOCH: {}, Train [Loss: {:.3f}, Accuracy: {:.3f}], Valid [Loss: {:.3f}, Accuracy: {:.3f}]]'.format(\n",
    "        epoch+1,\n",
    "        np.mean(losses_train),\n",
    "        acc_train/n_train,\n",
    "        np.mean(losses_valid),\n",
    "        acc_val/n_val,\n",
    "    ))\n",
    "\n",
    "\n",
    "x=np.arange(1,config['n_epochs']+1,1)\n",
    "y1=accuracy_train\n",
    "y2=cost_train\n",
    "y3 = accuracy_valid\n",
    "y4 = cost_valid\n",
    "c1,c2= 'blue', 'orange'\n",
    "l1,l2,l3,l4 = 'accuracy_train', 'cost_train','accuracy_valid','cost_valid'\n",
    "xl1, xl2= 'epochs', 'epochs'\n",
    "yl1, yl2= 'accuracy', 'cost'\n",
    "fig = plt.figure(figsize = (20,6))\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax1.plot(x, y1, color=c1, label=l1)\n",
    "ax1.plot(x,y3,color=c2,label=l3)\n",
    "ax2.plot(x, y2, color=c1, label=l2)\n",
    "ax2.plot(x,y4,color=c2,label=l4)\n",
    "ax1.set_xlabel(xl1)\n",
    "ax2.set_xlabel(xl2)\n",
    "ax1.set_ylabel(yl1)\n",
    "ax2.set_ylabel(yl2)\n",
    "ax1.legend(loc = 'upper right')\n",
    "ax2.legend(loc = 'upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "testset = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transforms.ToTensor() )\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "import pprint\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "prediction=[]\n",
    "label_list=[]\n",
    "# 勾配を記憶せず（学習せずに）に計算を行う\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device),data[1].to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        prediction.extend(predicted.tolist())\n",
    "        label_list.extend(labels.tolist())\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "\n",
    "metrix=confusion_matrix(prediction,label_list)\n",
    "print(classification_report(prediction,label_list))\n",
    "cmp = ConfusionMatrixDisplay(metrix)\n",
    "\n",
    "cmp.plot(cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_net2 = nn.Sequential(\n",
    "    nn.Conv2d(3, 32, 3),              # 32x32x3 -> 30x30x32\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(2),                  # 30x30x32 -> 15x15x32\n",
    "    nn.Conv2d(32, 64, 3),             # 15x15x32 -> 13x13x64\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(2),                  # 13x13x64 -> 6x6x64\n",
    "    nn.Conv2d(64, 128, 3),            # 6x6x64 -> 4x4x128\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(2),                  # 4x4x128 -> 2x2x128\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(2*2*128, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10)\n",
    ")\n",
    "\n",
    "\n",
    "def init_weights(m):  # Heの初期化\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.0)\n",
    "\n",
    "\n",
    "conv_net2.apply(init_weights)\n",
    "\n",
    "batch_size = 100\n",
    "n_epochs = 5\n",
    "lr = 0.01\n",
    "device = 'cuda'\n",
    "\n",
    "conv_net2.to(device)\n",
    "optimizer2 = optim.Adam(conv_net2.parameters(), lr=lr)\n",
    "loss_function = nn.CrossEntropyLoss()  # nn.ClossEntropyLossは，出力のsoftmax変換と，正解ラベルのone-hot vector化の機能を持っている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(config['n_epochs']):\n",
    "    losses_train = []\n",
    "    losses_valid = []\n",
    "\n",
    "    conv_net2.train()\n",
    "    n_train = 0\n",
    "    acc_train = 0\n",
    "    for x, t in dataloader_train:\n",
    "        n_train += t.size()[0]\n",
    "\n",
    "        conv_net2.zero_grad()  # 勾配の初期化\n",
    "\n",
    "        x = x.to(device)  # テンソルをGPUに移動\n",
    "        t = t.to(device)\n",
    "\n",
    "        y = conv_net2.forward(x)  # 順伝播\n",
    "\n",
    "        loss = loss_function(y, t)  # 誤差(クロスエントロピー誤差関数)の計算\n",
    "\n",
    "        loss.backward()  # 誤差の逆伝播\n",
    "\n",
    "        optimizer2.step()  # パラメータの更新\n",
    "\n",
    "        pred = y.argmax(1)  # 最大値を取るラベルを予測ラベルとする\n",
    "\n",
    "        acc_train += (pred == t).float().sum().item()\n",
    "        losses_train.append(loss.tolist())\n",
    "\n",
    "    conv_net2.eval()\n",
    "    n_val = 0\n",
    "    acc_val = 0\n",
    "    for x, t in dataloader_valid:\n",
    "        n_val += t.size()[0]\n",
    "\n",
    "        x = x.to(device)  # テンソルをGPUに移動\n",
    "        t = t.to(device)\n",
    "\n",
    "        y = conv_net2.forward(x)  # 順伝播\n",
    "\n",
    "        loss = loss_function(y, t)  # 誤差(クロスエントロピー誤差関数)の計算\n",
    "\n",
    "        pred = y.argmax(1)  # 最大値を取るラベルを予測ラベルとする\n",
    "\n",
    "        acc_val += (pred == t).float().sum().item()\n",
    "        losses_valid.append(loss.tolist())\n",
    "\n",
    "    print('EPOCH: {}, Train [Loss: {:.3f}, Accuracy: {:.3f}], Valid [Loss: {:.3f}, Accuracy: {:.3f}]'.format(\n",
    "        epoch+1,\n",
    "        np.mean(losses_train),\n",
    "        acc_train/n_train,\n",
    "        np.mean(losses_valid),\n",
    "        acc_val/n_val\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor() )\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "# 勾配を記憶せず（学習せずに）に計算を行う\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device),data[1].to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor() )\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# 勾配を記憶せず（学習せずに）に計算を行う\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = conv_net2(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
