{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6a0c13f2f0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config={\n",
    "    'BatchSize':256,\n",
    "    'seed':42,\n",
    "    'n_epochs' : 200,\n",
    "    'lr' : 0.001\n",
    "}\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data number:40000, Valid data number: 10000\n"
     ]
    }
   ],
   "source": [
    "trainval_dataset = datasets.CIFAR10('./data/cifar10', train=True, transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "# 前処理を定義\n",
    "transform = transforms.Compose([transforms.ToTensor(),])\n",
    "\n",
    "trainval_dataset = datasets.CIFAR10('./data/cifar10', train=True, transform=transform)\n",
    "\n",
    "# trainとvalidに分割\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(trainval_dataset, [len(trainval_dataset)-10000, 10000])\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['BatchSize'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataloader_valid = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['BatchSize'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"Train data number:{}, Valid data number: {}\".format(len(train_dataset), len(val_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuroGenesis1(nn.Module):\n",
    "    def __init__(self,linear_layer,threshold=0.1):\n",
    "        super(NeuroGenesis1,self).__init__()\n",
    "        self.layer=linear_layer\n",
    "        self.threshold=threshold\n",
    "\n",
    "    def generate_number(self,n):\n",
    "        numbers=[]\n",
    "        while len(numbers)<n:\n",
    "            randn = torch.randn(1)\n",
    "            if randn >= self.threshold:\n",
    "                numbers.append(randn.item())\n",
    "        return numbers            \n",
    "\n",
    "    def forward(self,x):\n",
    "        if self.training:\n",
    "            if epoch >3:\n",
    "                mask=torch.abs(self.layer.weight.data) < self.threshold\n",
    "                self.layer.weight.data[mask]=0.0\n",
    "                mask=self.layer.weight.data==0.0\n",
    "                self.layer.weight.data[mask]=torch.tensor(self.generate_number(mask.sum())).to(x.device)\n",
    "                print('weight is changed !')\n",
    "                return self.layer(x)\n",
    "            else:\n",
    "                return x\n",
    "            \n",
    "        else:\n",
    "            return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeurogenesisModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeurogenesisModel, self).__init__()\n",
    "        self.conv1=nn.Conv2d(3, 32, 3)       # 32x32x3 -> 30x30x32\n",
    "        self.av1=nn.ReLU()\n",
    "        self.pool1=nn.AvgPool2d(2)                  # 30x30x32 -> 15x15x32\n",
    "        self.conv2=nn.Conv2d(32, 64, 3)             # 15x15x32 -> 13x13x64\n",
    "        self.av2=nn.ReLU()\n",
    "        self.pool2=nn.AvgPool2d(2)                  # 13x13x64 -> 6x6x64               # 4x4x128 -> 2x2x128\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.fc1=nn.Linear(2304,256)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.genesis=NeuroGenesis1(linear_layer=self.fc1)\n",
    "        self.fc2=nn.Linear(256, 10)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.conv1(x)\n",
    "        x=self.av1(x)\n",
    "        x=self.pool1(x)\n",
    "        x=self.conv2(x)\n",
    "        x=self.av2(x)\n",
    "        x=self.pool2(x)     \n",
    "        x=self.flatten(x)\n",
    "        x=self.fc1(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.genesis(x)\n",
    "        x=self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model=NeurogenesisModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_weights(m):  # Heの初期化\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.0)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "device='cuda'\n",
    "model.to(device)\n",
    "optimizer2 = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "loss_function = nn.CrossEntropyLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1, Train [Loss: 1.615, Accuracy: 0.423], Valid [Loss: 1.374, Accuracy: 0.511]]\n",
      "EPOCH: 2, Train [Loss: 1.276, Accuracy: 0.546], Valid [Loss: 1.261, Accuracy: 0.554]]\n",
      "EPOCH: 3, Train [Loss: 1.144, Accuracy: 0.597], Valid [Loss: 1.123, Accuracy: 0.603]]\n",
      "EPOCH: 4, Train [Loss: 1.045, Accuracy: 0.635], Valid [Loss: 1.091, Accuracy: 0.613]]\n",
      "weight is changed !\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (256x256 and 2304x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/bdr/deep-learning/neurogenesis/ChangeValue.ipynb セル 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bdr/deep-learning/neurogenesis/ChangeValue.ipynb#Y456sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device)  \u001b[39m# テンソルをGPUに移動\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bdr/deep-learning/neurogenesis/ChangeValue.ipynb#Y456sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/bdr/deep-learning/neurogenesis/ChangeValue.ipynb#Y456sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m y \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(x)  \u001b[39m# 順伝播\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bdr/deep-learning/neurogenesis/ChangeValue.ipynb#Y456sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_function(y, t)  \u001b[39m# 誤差(クロスエントロピー誤差関数)の計算\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bdr/deep-learning/neurogenesis/ChangeValue.ipynb#Y456sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()  \u001b[39m# 誤差の逆伝播\u001b[39;00m\n",
      "\u001b[1;32m/home/bdr/deep-learning/neurogenesis/ChangeValue.ipynb セル 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bdr/deep-learning/neurogenesis/ChangeValue.ipynb#Y456sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bdr/deep-learning/neurogenesis/ChangeValue.ipynb#Y456sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/bdr/deep-learning/neurogenesis/ChangeValue.ipynb#Y456sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenesis(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bdr/deep-learning/neurogenesis/ChangeValue.ipynb#Y456sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bdr/deep-learning/neurogenesis/ChangeValue.ipynb#Y456sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/deep/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/bdr/deep-learning/neurogenesis/ChangeValue.ipynb セル 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bdr/deep-learning/neurogenesis/ChangeValue.ipynb#Y456sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdata[mask]\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mtensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_number(mask\u001b[39m.\u001b[39msum()))\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bdr/deep-learning/neurogenesis/ChangeValue.ipynb#Y456sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mweight is changed !\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/bdr/deep-learning/neurogenesis/ChangeValue.ipynb#Y456sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bdr/deep-learning/neurogenesis/ChangeValue.ipynb#Y456sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bdr/deep-learning/neurogenesis/ChangeValue.ipynb#Y456sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/deep/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/deep/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (256x256 and 2304x256)"
     ]
    }
   ],
   "source": [
    "accuracy_train=[]\n",
    "cost_train=[]\n",
    "accuracy_valid=[]\n",
    "cost_valid=[]\n",
    "\n",
    "for epoch in range(config['n_epochs']):\n",
    "    losses_train = []\n",
    "    losses_valid = []\n",
    "\n",
    "    model.train()\n",
    "    n_train = 0\n",
    "    acc_train = 0\n",
    "    for x, t in dataloader_train:\n",
    "        n_train += t.size()[0]\n",
    "\n",
    "        model.zero_grad()  # 勾配の初期化\n",
    "\n",
    "        x = x.to(device)  # テンソルをGPUに移動\n",
    "        t = t.to(device)\n",
    "\n",
    "        y = model.forward(x)  # 順伝播\n",
    "\n",
    "\n",
    "        loss = loss_function(y, t)  # 誤差(クロスエントロピー誤差関数)の計算\n",
    "\n",
    "        loss.backward()  # 誤差の逆伝播\n",
    "\n",
    "        optimizer2.step()  # パラメータの更新\n",
    "\n",
    "        pred = y.argmax(1)  # 最大値を取るラベルを予測ラベルとする\n",
    "\n",
    "        acc_train += (pred == t).float().sum().item()\n",
    "        losses_train.append(loss.tolist())\n",
    "\n",
    "    accuracy_train.append(acc_train/n_train)\n",
    "    cost_train.append(np.mean(losses_train))\n",
    "\n",
    "    model.eval()\n",
    "    n_val = 0\n",
    "    acc_val = 0\n",
    "    for x, t in dataloader_valid:\n",
    "        n_val += t.size()[0]\n",
    "\n",
    "        x = x.to(device)  # テンソルをGPUに移動\n",
    "        t = t.to(device)\n",
    "\n",
    "        y = model.forward(x)  # 順伝播\n",
    "\n",
    "        loss = loss_function(y, t)  # 誤差(クロスエントロピー誤差関数)の計算\n",
    "\n",
    "        pred = y.argmax(1)  # 最大値を取るラベルを予測ラベルとする\n",
    "\n",
    "        acc_val += (pred == t).float().sum().item()\n",
    "        losses_valid.append(loss.tolist())\n",
    "    accuracy_valid.append(acc_val/n_val)\n",
    "    cost_valid.append(np.mean(losses_valid))\n",
    "\n",
    "    print('EPOCH: {}, Train [Loss: {:.3f}, Accuracy: {:.3f}], Valid [Loss: {:.3f}, Accuracy: {:.3f}]]'.format(\n",
    "        epoch+1,\n",
    "        np.mean(losses_train),\n",
    "        acc_train/n_train,\n",
    "        np.mean(losses_valid),\n",
    "        acc_val/n_val,\n",
    "    ))\n",
    "\n",
    "\n",
    "x=np.arange(1,config['n_epochs']+1,1)\n",
    "y1=accuracy_train\n",
    "y2=cost_train\n",
    "y3 = accuracy_valid\n",
    "y4 = cost_valid\n",
    "c1,c2= 'blue', 'orange'\n",
    "l1,l2,l3,l4 = 'accuracy_train', 'cost_train','accuracy_valid','cost_valid'\n",
    "xl1, xl2= 'epochs', 'epochs'\n",
    "yl1, yl2= 'accuracy', 'cost'\n",
    "fig = plt.figure(figsize = (20,6))\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax1.plot(x, y1, color=c1, label=l1)\n",
    "ax1.plot(x,y3,color=c2,label=l3)\n",
    "ax2.plot(x, y2, color=c1, label=l2)\n",
    "ax2.plot(x,y4,color=c2,label=l4)\n",
    "ax1.set_xlabel(xl1)\n",
    "ax2.set_xlabel(xl2)\n",
    "ax1.set_ylabel(yl1)\n",
    "ax2.set_ylabel(yl2)\n",
    "ax1.legend(loc = 'upper right')\n",
    "ax2.legend(loc = 'upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def save_weights(model, filename):\n",
    "    weights = {}\n",
    "\n",
    "    # モデルの各層の名前とパラメータを取得\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            weights[name] = param.detach().cpu().numpy()\n",
    "\n",
    "    # .npz ファイルとして出力\n",
    "    np.savez(filename, **weights)\n",
    "\n",
    "save_weights(model, 'weights.npz')\n",
    "data = np.load('weights.npz')\n",
    "\n",
    "for key in data.keys():\n",
    "    print(key, data[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "testset = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transforms.ToTensor() )\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "import pprint\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "prediction=[]\n",
    "label_list=[]\n",
    "# 勾配を記憶せず（学習せずに）に計算を行う\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device),data[1].to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        prediction.extend(predicted.tolist())\n",
    "        label_list.extend(labels.tolist())\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "\n",
    "metrix=confusion_matrix(prediction,label_list)\n",
    "print(classification_report(prediction,label_list))\n",
    "cmp = ConfusionMatrixDisplay(metrix)\n",
    "\n",
    "cmp.plot(cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_net2 = nn.Sequential(\n",
    "    nn.Conv2d(3, 32, 3),              # 32x32x3 -> 30x30x32\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(2),                  # 30x30x32 -> 15x15x32\n",
    "    nn.Conv2d(32, 64, 3),             # 15x15x32 -> 13x13x64\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(2),                  # 13x13x64 -> 6x6x64\n",
    "    nn.Conv2d(64, 128, 3),            # 6x6x64 -> 4x4x128\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(2),                  # 4x4x128 -> 2x2x128\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(2*2*128, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10)\n",
    ")\n",
    "\n",
    "\n",
    "def init_weights(m):  # Heの初期化\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.0)\n",
    "\n",
    "\n",
    "conv_net2.apply(init_weights)\n",
    "\n",
    "batch_size = 100\n",
    "n_epochs = 5\n",
    "lr = 0.01\n",
    "device = 'cuda'\n",
    "\n",
    "conv_net2.to(device)\n",
    "optimizer2 = optim.Adam(conv_net2.parameters(), lr=lr)\n",
    "loss_function = nn.CrossEntropyLoss()  # nn.ClossEntropyLossは，出力のsoftmax変換と，正解ラベルのone-hot vector化の機能を持っている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(config['n_epochs']):\n",
    "    losses_train = []\n",
    "    losses_valid = []\n",
    "\n",
    "    conv_net2.train()\n",
    "    n_train = 0\n",
    "    acc_train = 0\n",
    "    for x, t in dataloader_train:\n",
    "        n_train += t.size()[0]\n",
    "\n",
    "        conv_net2.zero_grad()  # 勾配の初期化\n",
    "\n",
    "        x = x.to(device)  # テンソルをGPUに移動\n",
    "        t = t.to(device)\n",
    "\n",
    "        y = conv_net2.forward(x)  # 順伝播\n",
    "\n",
    "        loss = loss_function(y, t)  # 誤差(クロスエントロピー誤差関数)の計算\n",
    "\n",
    "        loss.backward()  # 誤差の逆伝播\n",
    "\n",
    "        optimizer2.step()  # パラメータの更新\n",
    "\n",
    "        pred = y.argmax(1)  # 最大値を取るラベルを予測ラベルとする\n",
    "\n",
    "        acc_train += (pred == t).float().sum().item()\n",
    "        losses_train.append(loss.tolist())\n",
    "\n",
    "    conv_net2.eval()\n",
    "    n_val = 0\n",
    "    acc_val = 0\n",
    "    for x, t in dataloader_valid:\n",
    "        n_val += t.size()[0]\n",
    "\n",
    "        x = x.to(device)  # テンソルをGPUに移動\n",
    "        t = t.to(device)\n",
    "\n",
    "        y = conv_net2.forward(x)  # 順伝播\n",
    "\n",
    "        loss = loss_function(y, t)  # 誤差(クロスエントロピー誤差関数)の計算\n",
    "\n",
    "        pred = y.argmax(1)  # 最大値を取るラベルを予測ラベルとする\n",
    "\n",
    "        acc_val += (pred == t).float().sum().item()\n",
    "        losses_valid.append(loss.tolist())\n",
    "\n",
    "    print('EPOCH: {}, Train [Loss: {:.3f}, Accuracy: {:.3f}], Valid [Loss: {:.3f}, Accuracy: {:.3f}]'.format(\n",
    "        epoch+1,\n",
    "        np.mean(losses_train),\n",
    "        acc_train/n_train,\n",
    "        np.mean(losses_valid),\n",
    "        acc_val/n_val\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor() )\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "# 勾配を記憶せず（学習せずに）に計算を行う\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device),data[1].to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor() )\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# 勾配を記憶せず（学習せずに）に計算を行う\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = conv_net2(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
