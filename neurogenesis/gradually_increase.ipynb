{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    'BatchSize':128,\n",
    "    'seed':42,\n",
    "    'n_epochs' : 200,\n",
    "    'lr' : 0.0001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Train data number:40000, Valid data number: 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainval_dataset = datasets.CIFAR10('../data/cifar10', train=True,download=True,transform=transforms.ToTensor())\n",
    "\n",
    "# 前処理を定義\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "trainval_dataset = datasets.CIFAR10('../data/cifar10', train=True, transform=transform)\n",
    "\n",
    "# trainとvalidに分割\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(trainval_dataset, [len(trainval_dataset)-10000, 10000],generator=torch.Generator().manual_seed(config['seed']))\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['BatchSize'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataloader_valid = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['BatchSize'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"Train data number:{}, Valid data number: {}\".format(len(train_dataset), len(val_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bio_genesis(nn.Module):\n",
    "    def __init__(self,ratio=0.5,path='checkpoint/gradually_increase.pt',patience=7,verbose=True):\n",
    "        super().__init__()\n",
    "        self.ratio=ratio\n",
    "        self.mask=True\n",
    "        self.patience=patience\n",
    "        self.best_acc=None\n",
    "        self.path=path\n",
    "        self.counter=0\n",
    "        self.__increase=False\n",
    "        self.verbose=verbose\n",
    "        self.val_acc_max= -np.Inf\n",
    "    @property\n",
    "    def bio_genesis(self):\n",
    "        return self.__increase\n",
    "    \n",
    "    def update(self,val_acc,model):\n",
    "        if self.best_acc is None:\n",
    "            self.best_acc=val_acc\n",
    "            self.save_checkpoint(model.val_acc)\n",
    "        elif self.val_acc < self.best_acc:\n",
    "            self.counter+=1\n",
    "            if self.verbose:\n",
    "                print(f'NeuroGensisCounter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.__increase = True\n",
    "                self.ratio -=1\n",
    "                print(f'Neuro Genesis Rario is {self.ratio}') \n",
    "        else:\n",
    "            self.best_acc = val_acc\n",
    "            self.save_checkpoint(model, val_acc)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self,model,val_acc):\n",
    "        if self.verbose:\n",
    "            print(f'Validation accuracy increased ({self.val_acc_max:.6f} --> {val_acc:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_acc_max = val_acc     \n",
    "\n",
    "    def load_checkpoint(self,model):\n",
    "        if self.verbose:\n",
    "            print(f'Loading model from last checkpoint with validation accuracy {self.val_acc_max:.6f}')\n",
    "        model.load_state_dict(torch.load(self.path))\n",
    "        return model    \n",
    "\n",
    "    def forward(self,x):\n",
    "        if self.training:\n",
    "                self.mask=torch.rand(*x.size()) > self.ratio\n",
    "                return x * self.mask.to(x.device)\n",
    "        else:\n",
    "            return x * (1.0 - self.ratio)\n",
    "\n",
    "genesis=Bio_genesis(patience=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeurogenesisModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeurogenesisModel, self).__init__()\n",
    "        self.conv1=nn.Conv2d(3, 32, 3)       # 32x32x3 -> 30x30x32\n",
    "        self.bn1=nn.BatchNorm2d(32)\n",
    "        self.av1=nn.ReLU()\n",
    "        self.pool1=nn.AvgPool2d(2)                  # 30x30x32 -> 15x15x32\n",
    "        self.conv2=nn.Conv2d(32, 64, 3)             # 15x15x32 -> 13x13x64\n",
    "        self.bn2=nn.BatchNorm2d(64)\n",
    "        self.av2=nn.ReLU()\n",
    "        self.pool2=nn.AvgPool2d(2)                  # 13x13x64 -> 6x6x64\n",
    "        self.conv3=nn.Conv2d(64, 128, 3)            # 6x6x64 -> 4x4x128\n",
    "        self.bn3=nn.BatchNorm2d(128)\n",
    "        self.av3=nn.ReLU()\n",
    "        self.pool3=nn.AvgPool2d(2)                  # 4x4x128 -> 2x2x128\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.fc1=nn.Linear(2*2*128, 256)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.genesis=genesis\n",
    "        self.fc2=nn.Linear(256, 10)\n",
    "        #self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        #self.relu = nn.ReLU()\n",
    "        #self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.conv1(x)\n",
    "        x=self.bn1(x)\n",
    "        x=self.av1(x)\n",
    "        x=self.pool1(x)\n",
    "        x=self.conv2(x)\n",
    "        x=self.bn2(x)\n",
    "        x=self.av2(x)\n",
    "        x=self.pool2(x)     \n",
    "        x=self.conv3(x)\n",
    "        x=self.bn3(x)\n",
    "        x=self.av3(x)\n",
    "        x=self.pool3(x)  \n",
    "        x=self.flatten(x)\n",
    "        x=self.fc1(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.genesis(x)\n",
    "        x=self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model=NeurogenesisModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_weights(m):  # Heの初期化\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.0)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "device='cuda'\n",
    "model.to(device)\n",
    "optimizer2 = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "loss_function = nn.CrossEntropyLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NeurogenesisModel' object has no attribute 'val_acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/bdr/machine-learning/neurogenesis/gradually_increase.ipynb セル 7\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bdr/machine-learning/neurogenesis/gradually_increase.ipynb#W6sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     accuracy_valid\u001b[39m.\u001b[39mappend(val_acc\u001b[39m/\u001b[39mn_val)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bdr/machine-learning/neurogenesis/gradually_increase.ipynb#W6sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     cost_valid\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mmean(losses_valid))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/bdr/machine-learning/neurogenesis/gradually_increase.ipynb#W6sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     genesis\u001b[39m.\u001b[39;49mupdate(val_acc\u001b[39m/\u001b[39;49mn_val, model)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bdr/machine-learning/neurogenesis/gradually_increase.ipynb#W6sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEPOCH: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, Train [Loss: \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m, Accuracy: \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m], Valid [Loss: \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m, Accuracy: \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m],ratio:\u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bdr/machine-learning/neurogenesis/gradually_increase.ipynb#W6sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m         epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bdr/machine-learning/neurogenesis/gradually_increase.ipynb#W6sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m         np\u001b[39m.\u001b[39mmean(losses_train),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bdr/machine-learning/neurogenesis/gradually_increase.ipynb#W6sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m         genesis\u001b[39m.\u001b[39mratio,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bdr/machine-learning/neurogenesis/gradually_increase.ipynb#W6sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     ))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bdr/machine-learning/neurogenesis/gradually_increase.ipynb#W6sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m model \u001b[39m=\u001b[39m genesis\u001b[39m.\u001b[39mload_checkpoint(model)\n",
      "\u001b[1;32m/home/bdr/machine-learning/neurogenesis/gradually_increase.ipynb セル 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bdr/machine-learning/neurogenesis/gradually_increase.ipynb#W6sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_acc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bdr/machine-learning/neurogenesis/gradually_increase.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_acc\u001b[39m=\u001b[39mval_acc\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/bdr/machine-learning/neurogenesis/gradually_increase.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_checkpoint(model\u001b[39m.\u001b[39;49mval_acc)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bdr/machine-learning/neurogenesis/gradually_increase.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_acc \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_acc:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bdr/machine-learning/neurogenesis/gradually_increase.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcounter\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/deep/lib/python3.11/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NeurogenesisModel' object has no attribute 'val_acc'"
     ]
    }
   ],
   "source": [
    "accuracy_train=[]\n",
    "cost_train=[]\n",
    "accuracy_valid=[]\n",
    "cost_valid=[]\n",
    "\n",
    "for epoch in range(config['n_epochs']):\n",
    "    losses_train = []\n",
    "    losses_valid = []\n",
    "\n",
    "    model.train()\n",
    "    n_train = 0\n",
    "    acc_train = 0\n",
    "    for x, t in dataloader_train:\n",
    "        n_train += t.size()[0]\n",
    "\n",
    "        model.zero_grad()  # 勾配の初期化\n",
    "\n",
    "        x = x.to(device)  # テンソルをGPUに移動\n",
    "        t = t.to(device)\n",
    "\n",
    "        y = model.forward(x)  # 順伝播\n",
    "\n",
    "\n",
    "        loss = loss_function(y, t)  # 誤差(クロスエントロピー誤差関数)の計算\n",
    "\n",
    "        loss.backward()  # 誤差の逆伝播\n",
    "\n",
    "        optimizer2.step()  # パラメータの更新\n",
    "\n",
    "        pred = y.argmax(1)  # 最大値を取るラベルを予測ラベルとする\n",
    "\n",
    "        acc_train += (pred == t).float().sum().item()\n",
    "        losses_train.append(loss.tolist())\n",
    "\n",
    "    accuracy_train.append(acc_train/n_train)\n",
    "    cost_train.append(np.mean(losses_train))\n",
    "\n",
    "    model.eval()\n",
    "    n_val = 0\n",
    "    val_acc = 0\n",
    "    for x, t in dataloader_valid:\n",
    "        n_val += t.size()[0]\n",
    "\n",
    "        x = x.to(device)  # テンソルをGPUに移動\n",
    "        t = t.to(device)\n",
    "\n",
    "        y = model.forward(x)  # 順伝播\n",
    "\n",
    "        loss = loss_function(y, t)  # 誤差(クロスエントロピー誤差関数)の計算\n",
    "\n",
    "        pred = y.argmax(1)  # 最大値を取るラベルを予測ラベルとする\n",
    "\n",
    "        val_acc += (pred == t).float().sum().item()\n",
    "        losses_valid.append(loss.tolist())\n",
    "    accuracy_valid.append(val_acc/n_val)\n",
    "    cost_valid.append(np.mean(losses_valid))\n",
    "\n",
    "\n",
    "    genesis.update(val_acc/n_val, model)\n",
    "\n",
    "    print('EPOCH: {}, Train [Loss: {:.3f}, Accuracy: {:.3f}], Valid [Loss: {:.3f}, Accuracy: {:.3f}],ratio:{:.3f}]'.format(\n",
    "        epoch+1,\n",
    "        np.mean(losses_train),\n",
    "        acc_train/n_train,\n",
    "        np.mean(losses_valid),\n",
    "        val_acc/n_val,\n",
    "        genesis.ratio,\n",
    "    ))\n",
    "\n",
    "\n",
    "\n",
    "model = genesis.load_checkpoint(model)\n",
    "\n",
    "y=len(accuracy_train)\n",
    "\n",
    "x=np.arange(1,y+1,1)\n",
    "y1=accuracy_train\n",
    "y2=cost_train\n",
    "y3 = accuracy_valid\n",
    "y4 = cost_valid\n",
    "c1,c2= 'blue', 'orange'\n",
    "l1,l2,l3,l4 = 'accuracy_train', 'cost_train','accuracy_valid','cost_valid'\n",
    "xl1, xl2= 'epochs', 'epochs'\n",
    "yl1, yl2= 'accuracy', 'cost'\n",
    "fig = plt.figure(figsize = (20,6))\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax1.plot(x, y1, color=c1, label=l1)\n",
    "ax1.plot(x,y3,color=c2,label=l3)\n",
    "ax2.plot(x, y2, color=c1, label=l2)\n",
    "ax2.plot(x,y4,color=c2,label=l4)\n",
    "ax1.set_xlabel(xl1)\n",
    "ax2.set_xlabel(xl2)\n",
    "ax1.set_ylabel(yl1)\n",
    "ax2.set_ylabel(yl2)\n",
    "ax1.legend(loc = 'upper right')\n",
    "ax2.legend(loc = 'upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor() )\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "# 勾配を記憶せず（学習せずに）に計算を行う\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device),data[1].to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
