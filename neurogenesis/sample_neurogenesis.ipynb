{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "import math\n",
    "import cnn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neurogenesisの関数の設定\n",
    "neurogenesisの手法\n",
    "* taeget_neurogenesis:正則化を施してノルムの重要度の低いものを消す\n",
    "1. 入力重み行列の形状を取得　weights.shape\n",
    "1. 各ニューロンの重要性をL1ノルムを計算して求める　torch.norm\n",
    "1. ドロップアウトするニューロンの数を指定　roundは小数点を切り落とす。weights_shape[0]で１番目の次元をとってくる。-1をすることでニューロンの数を１減らして0から始めるインデックスを使用する。\n",
    "1. 2で得たimportanceを3で得たidx番目まで値を残す\n",
    "1. 理論式でunimportance_maskは閾値未満のものがTrueになる。閾値以上のものがFalseになる\n",
    "1. dropout_maskは要素が１のものをdropoutして0に対応するものが出力が保持される　np.whereでunimportance_maskの中でTrueのものを取得。それをn_replace個だけランダムに取得。dropout_maskで値を全部0にする。出力に対応するインデックスに1を代入することで、dropputされる\n",
    "1. dropout_maskが0（ドロップアウトしない対象）に対応する位置の重みのみを保持して、ドロップアウトする対象に対応する位置の重みを削除します。結果として、ドロップアウトしない重みのみを保持した新しいweightsテンソルが得られる。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def flatten(t):\n",
    "    t = t.reshape(1, -1)\n",
    "    t = t.squeeze()\n",
    "    return t\n",
    "\n",
    "\n",
    "def targeted_neurogenesis(weights, n_replace, targeted_portion, is_training):\n",
    "    \"\"\"\n",
    "    Takes a weight matrix and applied targetted dropout based on weight\n",
    "    importance (From Gomez et al. 2019; https://for.ai/blog/targeted-dropout/)\n",
    "\n",
    "    Args:\n",
    "        weights - the input by ouput matrix of weights\n",
    "        dropout_rate - float (0,1), the proprotion of targeted neurons to dropout\n",
    "        targeted_portion - the proportion of neurons/weights to consider 'unimportant'\n",
    "            from which dropout_rate targets from\n",
    "        is_training - bool, whether model is training, or being evaluated\n",
    "    \"\"\"\n",
    "    # get the input vs output size\n",
    "    weights_shape = weights.shape\n",
    "\n",
    "    # l1-norm of neurons based on input weights to sort by importance\n",
    "    importance = torch.norm(weights, p=1, dim=1)\n",
    "\n",
    "    # chose number of indices to remove of the output neurons\n",
    "    idx = round(targeted_portion * weights_shape[0]) - 1\n",
    "\n",
    "    # when sorting the abs valued weights ascending order\n",
    "    # take the index of the targeted portion to get a threshold\n",
    "    importance_threshold = torch.sort(importance)[0][-idx] # TODO -idx\n",
    "\n",
    "    # only weights below threshold will be set to None\n",
    "    unimportance_mask = importance < importance_threshold  #TODO > change < regular\n",
    "\n",
    "    # during evaluation, only use important weights, without dropout threshold\n",
    "    if not is_training:\n",
    "       weights = torch.reshape(weights, weights_shape)\n",
    "       return weights\n",
    "\n",
    "    # difference between dropout_rate and unimportance_mask (i.e. threshold)\n",
    "    idx_drop = np.random.choice(np.where(unimportance_mask)[0], size=n_replace, replace=False)\n",
    "    dropout_mask = torch.zeros_like(unimportance_mask)\n",
    "    dropout_mask[idx_drop] = 1\n",
    "    \n",
    "    # delete dropped out units\n",
    "    weights = weights[~dropout_mask]\n",
    "\n",
    "    return weights, dropout_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# メインとなるモデルのNgn_CNNの箇所の説明\n",
    "## abrateの定義\n",
    "\n",
    "        if self.ablate:\n",
    "                if ix == 1:\n",
    "                    activation_size = x.size()[1]\n",
    "                    if self.ablation_mode == \"random\":\n",
    "                        ablate_size = int(self.ablation_prop * activation_size)\n",
    "                        indices = np.random.choice(\n",
    "                            range(activation_size),\n",
    "                            size=size,\n",
    "                            replace=False,\n",
    "                        )\n",
    "                    if self.ablation_mode == \"targetted\":\n",
    "                        indices = self.ablate_indices\n",
    "                    x[:, indices] = 0\n",
    "            if extract_layer == ix:\n",
    "                return x\n",
    "        x = self.fc3(x)\n",
    "\n",
    "- if self.abrate でabrate=Trueの時に以下のコマンドを実行することを明示する\n",
    "- if ix==1　で１番目の全結合層の時にこれを実行することを明示\n",
    "- x.size()[1]でxのテンソルの中で1次元目のものを取り出す\n",
    "- randomのモードの場合、alrate_size は予め決めていたabration_propにactivation_sizeをかけたもの\n",
    "- ablationの対象になるindexをランダムに取り出している。　targetedの場合は予め指定されていたablate_indicesが選ばれる。\n",
    "- x[:,indices]=0 で選ばれたindicesの要素を全て0にする。（重み行列の中で、indicesの列のものの全ての行を選択して0にする）\n",
    "- 実行が終わったらfc3(x)に0にしたものを代入する\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add_newで新しいニューロンを追加する方法\n",
    "\n",
    "\n",
    "    def add_new(\n",
    "        self,\n",
    "        p_new=0.01,\n",
    "        replace=True,\n",
    "        targeted_portion=None,\n",
    "        return_idx=False,\n",
    "        layer=1,\n",
    "    ):\n",
    "        \n",
    "        pnew: float, proportion of hidden layer to add\n",
    "        replace: float,Lina M. Tran  from 0-1 which is the proportion of new neurons that replace old neurons\n",
    "        target: bool, neurons that are lost are randomly chosen, or targetted\n",
    "                based on variance of activity\n",
    "        \n",
    "        # get a copy of current parameters\n",
    "        bias = [ix.bias.detach().clone().cpu() for ix in self.fcs]\n",
    "        current = [ix.weight.detach().clone().cpu() for ix in self.fcs]\n",
    "        if layer == 2:\n",
    "            current_fc3 = self.fc3.weight.detach().clone().cpu()\n",
    "\n",
    "##### 1. biasとcurrentのweightをコピーして、実際のパラメータに影響しないようにして変数をいじっていく。\n",
    "\n",
    "        # how many neurons to add?\n",
    "        if not p_new:\n",
    "            return\n",
    "        # if int given, use this as number of neurons to add\n",
    "        if (p_new % 1) == 0:\n",
    "            n_new = p_new\n",
    "        # if float given, use to calculate number of neurons to add\n",
    "        else:\n",
    "            n_new = int(self.layer_size * p_new)\n",
    "\n",
    "        if targeted_portion is not None:\n",
    "            targ_diff = round(targeted_portion * current[layer].shape[0]) - n_new\n",
    "            if targ_diff <= 0:\n",
    "                n_new = n_new + targ_diff - 3\n",
    "\n",
    "##### 2. ニューロンがなんこ追加されるかをいじっていく。p_newがなかったら操作をしない、整数値なら、その値をののまま使って適応する。小数ならば、整数値に変換して追加する。\n",
    "##### 3.targeted_portionがNoneの時に実際に減らすニューロンの数を指定する。targ_diffを決定する。roundは小数点四捨五入するメソッド\n",
    "\n",
    "        self.n_new = n_new\n",
    "        n_replace = n_new if replace else 0  # number lost\n",
    "        difference = n_new - n_replace  # net addition or loss\n",
    "        self.layer_size += difference  # final layer size\n",
    "\n",
    "##### 4. self.n_new, n_replace,differnce,self.layer_sizeを定義している。　n_replaceはreplaceがtrueの時にn_newの値になる　Falseの時に0\n",
    "\n",
    "        # reallocate the weights and biases\n",
    "        if replace:\n",
    "            # if some neurons are being removed\n",
    "            if targeted_portion is not None:\n",
    "                try:\n",
    "                    weights, mask = targeted_neurogenesis(\n",
    "                        current[layer], n_replace, targeted_portion, self.training\n",
    "                    )\n",
    "                except ValueError:\n",
    "                    print(\n",
    "                        \"n_replace\",\n",
    "                        n_replace,\n",
    "                        \"targ\",\n",
    "                        targeted_portion * (current[layer].shape[0]),\n",
    "                    )\n",
    "\n",
    "                # if neurons are targetted for removal\n",
    "                idx = np.where(mask)[0]\n",
    "                bias[1] = np.delete(bias[1], idx)\n",
    "                current[layer] = np.delete(current[layer], idx, axis=0)\n",
    "                current[layer + 1] = np.delete(current[layer + 1], idx, axis=1)\n",
    "            else:\n",
    "                # if neurons are randomly chosen for removal\n",
    "                idx = np.random.choice(\n",
    "                    range(current[layer].shape[0]), size=n_replace, replace=False\n",
    "                )\n",
    "\n",
    "                # delete idx neurons from bias and current weights (middle layer)\n",
    "                bias[1] = np.delete(bias[1], idx)\n",
    "                current[layer] = np.delete(current[layer], idx, axis=0)\n",
    "                try:\n",
    "                    current[layer + 1] = np.delete(current[layer + 1], idx, axis=1)\n",
    "                except IndexError:\n",
    "                    current_fc3 = np.delete(current_fc3, idx, axis=1)\n",
    "\n",
    "\n",
    "            self.idx = idx\n",
    "##### 5.これ以前は改修中\n",
    "        # create new weight shapes\n",
    "        w_in = torch.Tensor(\n",
    "            self.layer_size,\n",
    "            current[layer].shape[1],\n",
    "        )\n",
    "        b_in = torch.Tensor(self.layer_size)\n",
    "        if layer < 2:\n",
    "            w_out = torch.Tensor(\n",
    "                current[layer + 1].shape[0],\n",
    "                self.layer_size,\n",
    "            )\n",
    "        elif layer == 2:\n",
    "            w_out = torch.Tensor(\n",
    "                current_fc3.shape[0],\n",
    "                self.layer_size,\n",
    "            )\n",
    "##### 6. toech.Tensorで新しくパラメータを作る。torch.Tensorの引数が新しいパラメータの形状。<br>　　b_inはレイヤー全体で同じパラメータになるので引数はself.layer_sizeになる\n",
    "        # initialize new weights\n",
    "        nn.init.kaiming_uniform_(w_in, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(w_out, a=math.sqrt(5))\n",
    "\n",
    "        # in bias (out bias unaffected by neurogenesis)\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(w_in)\n",
    "        bound = 1 / math.sqrt(fan_in)\n",
    "        nn.init.uniform_(b_in, -bound, bound)\n",
    "\n",
    "##### 7. Heの初期化でそれぞれのパラメータを初期化しているみたい。これまじでわかりにくいので改善の余地あり\n",
    "\n",
    "        # put back current bias and weights into newly initiliazed layers\n",
    "        b_in[:-n_new] = bias[1]\n",
    "        w_in[:-n_new, :] = current[layer]\n",
    "        if layer == 2:\n",
    "            w_out[:, :-n_new] = current_fc3\n",
    "        else:\n",
    "            w_out[:, :-n_new] = current[layer + 1]\n",
    "##### 8.b_in[:-n_new]はb_inのパラメータの中からn_newを取り除いたものであり、これをすることで全てのb_inの中から古いからパラメータをb_inに格納して<br>新しいものは値を変えるように設定できる\n",
    "        # create the parameters again\n",
    "        self.fcs[layer].bias = nn.Parameter(b_in)\n",
    "        self.fcs[layer].weight = nn.Parameter(w_in)\n",
    "        if layer == 2:\n",
    "            self.fc3.weight = nn.Parameter(w_out)\n",
    "        else:\n",
    "            self.fcs[layer + 1].weight = nn.Parameter(w_out)\n",
    "\n",
    "        # need to send all the data to GPU again\n",
    "        self.fcs.to(dev)\n",
    "        if layer == 2:\n",
    "            self.fc3.to(dev)\n",
    "\n",
    "        if return_idx and (n_replace > 0):\n",
    "            return idx\n",
    "##### 9.この項は改装中\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import Index\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import targeted_neurogenesis\n",
    "\n",
    "dev = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def load_data(\n",
    "    mode,\n",
    "    data_folder=\"./data\",\n",
    "    num_workers=16,\n",
    "    batch_size=50,\n",
    "    split=0.1,\n",
    "    seed=23,\n",
    "    fashion=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Helper function to read in image dataset, and split into\n",
    "    training, validation and test sets.\n",
    "    ===\n",
    "    mode: str, ['validation', 'test]. If 'validation', training data\n",
    "         will be divided based on split parameter.\n",
    "         If test, .valid = None, and all training data is used for training\n",
    "    split: float, where 0 < split < 1. Where train = split * num_samples\n",
    "        and valid = (1 - split) * num_samples\n",
    "    seed: int, random seed to generate validation/training split\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )\n",
    "    assert mode in [\"validation\", \"test\"]\n",
    "\n",
    "    if fashion:\n",
    "        trainset = torchvision.datasets.MNIST(\n",
    "            data_folder,\n",
    "            train=True,\n",
    "            transform=transforms.Compose(\n",
    "                [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    "            ),\n",
    "        )\n",
    "        testset = torchvision.datasets.MNIST(\n",
    "            data_folder,\n",
    "            train=False,\n",
    "            transform=transforms.Compose(\n",
    "                [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    "            ),\n",
    "        )\n",
    "        print(\"Loaded FMNIST dataset\")\n",
    "    else:\n",
    "        trainset = torchvision.datasets.CIFAR10(\n",
    "            root=data_folder, train=True, download=False, transform=transform\n",
    "        )\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR10(\n",
    "            root=data_folder, train=False, download=False, transform=transform\n",
    "        )\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset,\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    if mode == \"validation\":\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        num_train = 50000\n",
    "        indices = list(range(num_train))\n",
    "\n",
    "        train_idx, valid_idx = train_test_split(\n",
    "            indices, test_size=split, random_state=seed\n",
    "        )\n",
    "\n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "        trainloader = torch.utils.data.DataLoader(\n",
    "            trainset,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=True,\n",
    "            sampler=train_sampler,\n",
    "        )\n",
    "\n",
    "        validloader = torch.utils.data.DataLoader(\n",
    "            trainset,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers,\n",
    "            sampler=valid_sampler,\n",
    "            drop_last=True,\n",
    "        )\n",
    "        print(\"Created data loaders\")\n",
    "        return trainloader, validloader, testloader\n",
    "\n",
    "    elif mode == \"test\":\n",
    "        trainloader = torch.utils.data.DataLoader(\n",
    "            trainset,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "        print(\"Created data loaders\")\n",
    "        return trainloader, testloader\n",
    "\n",
    "\n",
    "class Cifar10_data(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode=\"validation\",\n",
    "        data_folder=\"./data\",\n",
    "        batch_size=50,\n",
    "        fashion=False,\n",
    "        num_workers=16,\n",
    "        split=0.1,\n",
    "        seed=23,\n",
    "    ):\n",
    "        if mode == \"validation\":\n",
    "            self.train, self.valid, self.test = load_data(\n",
    "                mode=mode,\n",
    "                data_folder=data_folder,\n",
    "                batch_size=batch_size,\n",
    "                num_workers=num_workers,\n",
    "                split=split,\n",
    "                fashion=fashion,\n",
    "                seed=seed,\n",
    "            )\n",
    "        elif mode == \"test\":\n",
    "            self.train, self.test = load_data(\n",
    "                mode=mode,\n",
    "                data_folder=data_folder,\n",
    "                seed=seed,\n",
    "                fashion=fashion,\n",
    "                batch_size=batch_size,\n",
    "                num_workers=num_workers,\n",
    "            )\n",
    "            self.valid = None\n",
    "\n",
    "\n",
    "def early_stopping(starting, patience, count, best_score, prediction):\n",
    "    # starting accuracy (in case network is not training at all)\n",
    "    if starting is None:\n",
    "        starting = prediction[\"Accuracy\"][0]\n",
    "    # first epoch\n",
    "    if best_score is None:\n",
    "        best_score = prediction[\"Loss\"][0]\n",
    "    # if score is decreasing, start counter\n",
    "    elif np.round(prediction[\"Loss\"][0], 4) < best_score:\n",
    "        count = 0\n",
    "        best_score = prediction[\"Loss\"][0]\n",
    "        return count, best_score\n",
    "    else:\n",
    "        # if we've reached patience threshold, end training\n",
    "        count += 1\n",
    "        if count > patience:\n",
    "            return\n",
    "        # network is not training\n",
    "        elif prediction[\"Accuracy\"][0] < (starting):\n",
    "            return\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    dataset,\n",
    "    epochs=15,\n",
    "    device=dev,\n",
    "    dtype=torch.float,\n",
    "    neurogenesis=None,\n",
    "    optim_fn=optim.Adam,\n",
    "    optim_args={\"lr\": 0.0002},\n",
    "    turnover=True,\n",
    "    frequency=0,\n",
    "    excite=False,\n",
    "    end_neurogenesis=8,\n",
    "    early_stop=True,\n",
    "    patience=2,\n",
    "    checkpoint=False,\n",
    "    layer=1,\n",
    "    targeted_portion=None,\n",
    "    **kwargs,\n",
    "):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim_fn(model.parameters(), **optim_args)\n",
    "\n",
    "    log = np.zeros(epochs)\n",
    "    best_score = None\n",
    "    starting = None\n",
    "    count = 0\n",
    "\n",
    "    # neurogenesis\n",
    "    epoch_neurogenesis = False\n",
    "    batch_neurogenesis = False\n",
    "\n",
    "    if (neurogenesis is not None) and (neurogenesis):\n",
    "        if frequency:\n",
    "            batch_neurogenesis = True\n",
    "        else:\n",
    "            epoch_neurogenesis = True\n",
    "    idx_list = set()\n",
    "\n",
    "    if excite is not None and excite > 0:\n",
    "        model.excite = excite\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        model.train()\n",
    "\n",
    "        if epoch >= end_neurogenesis:\n",
    "            epoch_neurogenesis = False\n",
    "            batch_neurogenesis = False\n",
    "            model.excite = False\n",
    "        for i, data in enumerate(dataset.train, 0):\n",
    "            if batch_neurogenesis:\n",
    "                if (epoch % frequency) == 0:\n",
    "                    ngn_idx = model.add_new(neurogenesis, turnover, targeted_portion, layer=layer, return_idx=True)\n",
    "                    idx_list.update(ngn_idx)\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # If validation set exists, predict on validation set\n",
    "        # Otherwise use the test set\n",
    "        if dataset.valid is not None:\n",
    "            prediction = predict(model, dataset, True, get_loss=False)\n",
    "        elif dataset.valid is None:\n",
    "            prediction = predict(model, dataset, False, get_loss=False)\n",
    "\n",
    "        log[epoch] = prediction[\"Accuracy\"][0]\n",
    "        if epoch_neurogenesis:\n",
    "            idx = model.add_new(neurogenesis, turnover, targeted_portion, layer=layer, return_idx=True)\n",
    "            idx_list.append(idx)\n",
    "            if not turnover:  # add new parameters\n",
    "                optimizer.add_param_group(\n",
    "                    {\n",
    "                        \"params\": model.fc_new_in[-1].parameters(),\n",
    "                        \"lr\": optim_args[\"lr\"],\n",
    "                        \"momentum\": optim_args[\"momentum\"],\n",
    "                    }\n",
    "                )\n",
    "                optimizer.add_param_group(\n",
    "                    {\n",
    "                        \"params\": model.fc_new_out[-1].parameters(),\n",
    "                        \"lr\": optim_args[\"lr\"],\n",
    "                        \"momentum\": optim_args[\"momentum\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return list(log), optimizer\n",
    "\n",
    "\n",
    "def predict(model, dataset, valid=False, train=False, device=dev, get_loss=False):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    correct = []\n",
    "    total = 0\n",
    "    losses = []\n",
    "\n",
    "    model.eval()\n",
    "    # use the correct dataset\n",
    "    if valid:\n",
    "        try:\n",
    "            loader = dataset.valid\n",
    "        except AttributeError:\n",
    "            print(\"No validation set. You are in test mode.\")\n",
    "            return\n",
    "    elif train:\n",
    "        loader = dataset.train\n",
    "    else:\n",
    "        loader = dataset.test\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            # calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # calculate accuracy (do not use softmax)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct.append((predicted == labels).sum().item())\n",
    "\n",
    "    avg_loss = np.array(losses).mean()\n",
    "    sem_loss = stats.sem(np.array(losses))\n",
    "\n",
    "    accuracy = 100 * float(np.array(correct).sum()) / total\n",
    "    sem_accuracy = 0\n",
    "\n",
    "    #    return accuracy, avg_loss\n",
    "    return {\"Loss\": (avg_loss, sem_loss), \"Accuracy\": (accuracy, sem_accuracy)}\n",
    "\n",
    "def error_types(model, dataset, device=dev):\n",
    "    correct = []\n",
    "    total = 0\n",
    "    losses = []\n",
    "\n",
    "    model.eval()\n",
    "    # use the correct dataset\n",
    "\n",
    "    loader = dataset.test\n",
    "\n",
    "\n",
    "    predictions, actual = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            # calculate accuracy (do not use softmax)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            actual.append(labels.cpu().numpy())\n",
    "            predictions.append(predicted.cpu().numpy())\n",
    "\n",
    "    return np.array(list(zip(predictions, actual))).T\n",
    "\n",
    "\n",
    "def ablate_targetted(model, dataset, indices):\n",
    "    model.ablate = True\n",
    "    model.ablate_indices = indices\n",
    "    model.ablation_mode = \"targetted\"\n",
    "    acc = predict(model, dataset)\n",
    "    result = acc[\"Accuracy\"][0]\n",
    "    return result\n",
    "\n",
    "\n",
    "def ablation(model, dataset, mode=\"random\", step=0.05):\n",
    "    \"\"\"\n",
    "    layer: layers to remove neurons\n",
    "    proportion: float, fraction of neurons to ablate\n",
    "    \"\"\"\n",
    "    assert mode in [\"random\", \"targetted\"], \"mode must be random or targetted\"\n",
    "    model.ablate = True\n",
    "    proportions = np.arange(0, 1 + step, step)\n",
    "    results = np.zeros((len(proportions), 2))\n",
    "\n",
    "    counter = 0\n",
    "    for prop in proportions:\n",
    "        model.ablation_prop = prop\n",
    "        model.ablation_mode = mode\n",
    "        acc = predict(model, dataset, train=True)\n",
    "        result[counter] = (prop, acc[\"Accuracy\"][0])\n",
    "        counter += 1\n",
    "\n",
    "    model.ablate = False\n",
    "    return result\n",
    "\n",
    "\n",
    "class NgnCnn(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_size=250,\n",
    "        channels=3,\n",
    "        control=False,\n",
    "        seed=0,\n",
    "        excite=False,\n",
    "        neural_noise=None,\n",
    "    ):\n",
    "        torch.manual_seed(seed)\n",
    "        super(NgnCnn, self).__init__()\n",
    "        # parameters\n",
    "        self.ablate = False\n",
    "        self.dropout = 0\n",
    "        self.channels = channels\n",
    "        self.excite = excite\n",
    "        self.n_new = 0\n",
    "        self.control = False\n",
    "        if self.conZrol:\n",
    "            self.idx_control = np.random.choice(\n",
    "                range(layer_size), size=8, replace=False\n",
    "            )\n",
    "        self.neural_noise = neural_noise\n",
    "\n",
    "        # 3@16x16\n",
    "        self.conv1 = nn.Conv2d(channels, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.pool4 = nn.AvgPool2d(kernel_size=1, stride=1)\n",
    "\n",
    "        self.layer_size = layer_size\n",
    "\n",
    "        self.fc_new_in = nn.ModuleList()\n",
    "        self.fc_new_out = nn.ModuleList()\n",
    "\n",
    "        if self.channels == 3:\n",
    "            self.cnn_output = 64 * 4 * 4\n",
    "        elif self.channels == 1:\n",
    "            self.cnn_output = 64 * 9\n",
    "        # three fully connected layers\n",
    "        self.fcs = nn.ModuleList(\n",
    "            [\n",
    "                nn.Linear(self.cnn_output, self.layer_size),  # 0\n",
    "                nn.Linear(self.layer_size, self.layer_size),  # 1 on dim 2 neurogenesis\n",
    "                nn.Linear(self.layer_size, self.layer_size),  # 2\n",
    "            ]\n",
    "        )\n",
    "        self.fc3 = nn.Linear(self.layer_size, 10, bias=False)\n",
    "\n",
    "    def forward(self, x, extract_layer=None):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        x = x.view(-1, self.cnn_output)\n",
    "\n",
    "        for ix, fc in enumerate(self.fcs):\n",
    "            x = fc(x)\n",
    "            if self.neural_noise is not None and ix == 0 and self.training:\n",
    "                mean, std = self.neural_noise\n",
    "                noise = torch.zeros_like(x, device=dev)\n",
    "                noise = noise.log_normal_(mean=mean, std=std)\n",
    "                x = x * noise\n",
    "            x = F.relu(x)\n",
    "\n",
    "            if self.excite and ix == 1 and self.n_new and self.training:\n",
    "                idx = self.idx_control if self.control else self.idx\n",
    "                excite_mask = torch.ones_like(x)\n",
    "                excite_mask[:, idx] = self.excite\n",
    "                excite_mask.to(dev)\n",
    "                x = x * excite_mask\n",
    "\n",
    "            if self.dropout:\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "                x = torch.renorm(x, 1, 1, 3)  # max norm\n",
    "\n",
    "            # for ablation experiments\n",
    "            if self.ablate:\n",
    "                if ix == 1:\n",
    "                    activation_size = x.size()[1]\n",
    "                    if self.ablation_mode == \"random\":\n",
    "                        ablate_size = int(self.ablation_prop * activation_size)\n",
    "                        indices = np.random.choice(\n",
    "                            range(activation_size),\n",
    "                            size=size,\n",
    "                            replace=False,\n",
    "                        )\n",
    "                    if self.ablation_mode == \"targetted\":\n",
    "                        indices = self.ablate_indices\n",
    "                    x[:, indices] = 0\n",
    "            if extract_layer == ix:\n",
    "                return x\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def add_new(\n",
    "        self,\n",
    "        p_new=0.01,\n",
    "        replace=True,\n",
    "        targeted_portion=None,\n",
    "        return_idx=False,\n",
    "        layer=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        pnew: float, proportion of hidden layer to add\n",
    "        replace: float,Lina M. Tran  from 0-1 which is the proportion of new neurons that replace old neurons\n",
    "        target: bool, neurons that are lost are randomly chosen, or targetted\n",
    "                based on variance of activity\n",
    "        \"\"\"\n",
    "        # get a copy of current parameters\n",
    "        bias = [ix.bias.detach().clone().cpu() for ix in self.fcs]\n",
    "        current = [ix.weight.detach().clone().cpu() for ix in self.fcs]\n",
    "        if layer == 2:\n",
    "            current_fc3 = self.fc3.weight.detach().clone().cpu()\n",
    "\n",
    "        # how many neurons to add?\n",
    "        if not p_new:\n",
    "            return\n",
    "        # if int given, use this as number of neurons to add\n",
    "        if (p_new % 1) == 0:\n",
    "            n_new = p_new\n",
    "        # if float given, use to calculate number of neurons to add\n",
    "        else:\n",
    "            n_new = int(self.layer_size * p_new)\n",
    "\n",
    "        if targeted_portion is not None:\n",
    "            targ_diff = round(targeted_portion * current[layer].shape[0]) - n_new\n",
    "            if targ_diff <= 0:\n",
    "                n_new = n_new + targ_diff - 3\n",
    "\n",
    "        self.n_new = n_new\n",
    "        n_replace = n_new if replace else 0  # number lost\n",
    "        difference = n_new - n_replace  # net addition or loss\n",
    "        self.layer_size += difference  # final layer size\n",
    "\n",
    "        # reallocate the weights and biases\n",
    "        if replace:\n",
    "            # if some neurons are being removed\n",
    "            if targeted_portion is not None:\n",
    "                try:\n",
    "                    weights, mask = targeted_neurogenesis(\n",
    "                        current[layer], n_replace, targeted_portion, self.training\n",
    "                    )\n",
    "                except ValueError:\n",
    "                    print(\n",
    "                        \"n_replace\",\n",
    "                        n_replace,\n",
    "                        \"targ\",\n",
    "                        targeted_portion * (current[layer].shape[0]),\n",
    "                    )\n",
    "\n",
    "                # if neurons are targetted for removal\n",
    "                idx = np.where(mask)[0]\n",
    "                bias[1] = np.delete(bias[1], idx)\n",
    "                current[layer] = np.delete(current[layer], idx, axis=0)\n",
    "                current[layer + 1] = np.delete(current[layer + 1], idx, axis=1)\n",
    "            else:\n",
    "                # if neurons are randomly chosen for removal\n",
    "                idx = np.random.choice(\n",
    "                    range(current[layer].shape[0]), size=n_replace, replace=False\n",
    "                )\n",
    "\n",
    "                # delete idx neurons from bias and current weights (middle layer)\n",
    "                bias[1] = np.delete(bias[1], idx)\n",
    "                current[layer] = np.delete(current[layer], idx, axis=0)\n",
    "                try:\n",
    "                    current[layer + 1] = np.delete(current[layer + 1], idx, axis=1)\n",
    "                except IndexError:\n",
    "                    current_fc3 = np.delete(current_fc3, idx, axis=1)\n",
    "\n",
    "\n",
    "            self.idx = idx\n",
    "\n",
    "        # create new weight shapes\n",
    "        w_in = torch.Tensor(\n",
    "            self.layer_size,\n",
    "            current[layer].shape[1],\n",
    "        )\n",
    "        b_in = torch.Tensor(self.layer_size)\n",
    "        if layer < 2:\n",
    "            w_out = torch.Tensor(\n",
    "                current[layer + 1].shape[0],\n",
    "                self.layer_size,\n",
    "            )\n",
    "        elif layer == 2:\n",
    "            w_out = torch.Tensor(\n",
    "                current_fc3.shape[0],\n",
    "                self.layer_size,\n",
    "            )\n",
    "\n",
    "        # initialize new weights\n",
    "        nn.init.kaiming_uniform_(w_in, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(w_out, a=math.sqrt(5))\n",
    "\n",
    "        # in bias (out bias unaffected by neurogenesis)\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(w_in)\n",
    "        bound = 1 / math.sqrt(fan_in)\n",
    "        nn.init.uniform_(b_in, -bound, bound)\n",
    "\n",
    "        # put back current bias and weights into newly initiliazed layers\n",
    "        b_in[:-n_new] = bias[1]\n",
    "        w_in[:-n_new, :] = current[layer]\n",
    "        if layer == 2:\n",
    "            w_out[:, :-n_new] = current_fc3\n",
    "        else:\n",
    "            w_out[:, :-n_new] = current[layer + 1]\n",
    "\n",
    "        # create the parameters again\n",
    "        self.fcs[layer].bias = nn.Parameter(b_in)\n",
    "        self.fcs[layer].weight = nn.Parameter(w_in)\n",
    "        if layer == 2:\n",
    "            self.fc3.weight = nn.Parameter(w_out)\n",
    "        else:\n",
    "            self.fcs[layer + 1].weight = nn.Parameter(w_out)\n",
    "\n",
    "        # need to send all the data to GPU again\n",
    "        self.fcs.to(dev)\n",
    "        if layer == 2:\n",
    "            self.fc3.to(dev)\n",
    "\n",
    "        if return_idx and (n_replace > 0):\n",
    "            return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablate_targetted(model, dataset, indices):\n",
    "    model.ablate = True\n",
    "    model.ablate_indices = indices\n",
    "    model.ablation_mode = \"targetted\"\n",
    "    acc = predict(model, dataset)\n",
    "    result = acc[\"Accuracy\"][0]\n",
    "    return result\n",
    "\n",
    "\n",
    "def ablation(model, dataset, mode=\"random\", step=0.05):\n",
    "    \"\"\"\n",
    "    layer: layers to remove neurons\n",
    "    proportion: float, fraction of neurons to ablate\n",
    "    \"\"\"\n",
    "    assert mode in [\"random\", \"targetted\"], \"mode must be random or targetted\"\n",
    "    model.ablate = True\n",
    "    proportions = np.arange(0, 1 + step, step)\n",
    "    results = np.zeros((len(proportions), 2))\n",
    "\n",
    "    counter = 0\n",
    "    for prop in proportions:\n",
    "        model.ablation_prop = prop\n",
    "        model.ablation_mode = mode\n",
    "        acc = predict(model, dataset, train=True)\n",
    "        results[counter] = (prop, acc[\"Accuracy\"][0])\n",
    "        counter += 1\n",
    "\n",
    "    model.ablate = False\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytest'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpytest\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m@pytest\u001b[39m\u001b[39m.\u001b[39mfixture()\n\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmodel\u001b[39m(request):\n\u001b[1;32m      5\u001b[0m     mdl \u001b[39m=\u001b[39m cnn\u001b[39m.\u001b[39mNgnCnn(layer_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytest'"
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.fixture()\n",
    "def model(request):\n",
    "    mdl = cnn.NgnCnn(layer_size=10)\n",
    "    mdl.to(device)\n",
    "\n",
    "    def fin():\n",
    "        print(\"Teardown model\")\n",
    "\n",
    "    request.addfinalizer(fin)\n",
    "    return mdl\n",
    "\n",
    "\n",
    "@pytest.fixture()\n",
    "def model_ngn(request, model):\n",
    "    p_new = 0.5\n",
    "    replace = 0.6\n",
    "    model.add_new(p_new=p_new, replace=replace)\n",
    "\n",
    "    def fin():\n",
    "        print(\"Teardown model\")\n",
    "\n",
    "    request.addfinalizer(fin)\n",
    "    return model\n",
    "\n",
    "\n",
    "@pytest.fixture()\n",
    "def dataset(request):\n",
    "    dt = cnn.Cifar10_data()\n",
    "\n",
    "    def fin():\n",
    "        print(\"Teardown dataset\")\n",
    "\n",
    "    request.addfinalizer(fin)\n",
    "    return dt\n",
    "\n",
    "\n",
    "def test_load_data_valid_split():\n",
    "    split = 0.2\n",
    "    batch_size = 4\n",
    "    num_samples = 50000\n",
    "    num_valid = int(50000 * split)\n",
    "    print(num_valid)\n",
    "    num_train = num_samples - int(num_samples * split)\n",
    "    train, valid, test = cnn.load_data(\"validation\", split=split)\n",
    "    assert len(train) * batch_size == num_train\n",
    "    assert len(valid) * batch_size == num_valid\n",
    "\n",
    "\n",
    "def test_model_updates(model, dataset):\n",
    "    before = list(model.parameters())[0].clone().detach().cpu().numpy()\n",
    "    cnn.train_model(model, dataset, epochs=1)\n",
    "    after = list(model.parameters())[0].clone().detach().cpu().numpy()\n",
    "    for b, a in zip(before, after):\n",
    "        # Make sure something changed.\n",
    "        assert (b != a).any()\n",
    "\n",
    "\n",
    "def test_neurogenesis_turnover(request, model):\n",
    "    p_new = 5\n",
    "    replace = 0.6\n",
    "    added = int(p_new * (1 - replace))\n",
    "    layer_size = model.layer_size\n",
    "\n",
    "    before = model.fcs[1].weight.shape[0]\n",
    "    model.add_new(p_new=p_new, replace=replace)\n",
    "    after = model.fcs[1].weight.shape[0]\n",
    "\n",
    "    assert (\n",
    "        after - before\n",
    "    ) == added, \"Difference between layer sizes before and after neurogenesis does not equal net addition\"\n",
    "    assert after == (\n",
    "        layer_size + added\n",
    "    ), \"Final size after neurogenesis not original size + net added\"\n",
    "\n",
    "\n",
    "def test_neurogenesis_kept_replacement(model):\n",
    "    \"\"\"\n",
    "    Test whether\n",
    "    \"\"\"\n",
    "    p_new = 5\n",
    "    replace = 0.6\n",
    "    #    removed = int(p_new * replace)\n",
    "\n",
    "    before = model.fcs[1].weight.clone().cpu().data.numpy()\n",
    "    idx = model.add_new(p_new=p_new, replace=replace, return_idx=True)\n",
    "    after = model.fcs[1].weight.clone().cpu().data.numpy()\n",
    "    before = np.delete(before, idx, axis=0)\n",
    "    assert (before == after[:-p_new]).all()\n",
    "\n",
    "\n",
    "def test_neurogenesis_kept_no_replacement(model):\n",
    "    p_new = 5\n",
    "    replace = 0\n",
    "\n",
    "    before = model.fcs[1].weight.clone()\n",
    "    model.add_new(p_new=p_new, replace=replace)\n",
    "    after = model.fcs[1].weight.clone()\n",
    "\n",
    "    assert (before == after[:-p_new]).all()\n",
    "\n",
    "\n",
    "def test_model_updates_post_neurogenesis(model_ngn, dataset):\n",
    "    before = list(model_ngn.parameters())[0].clone().detach().cpu().numpy()\n",
    "    cnn.train_model(model_ngn, dataset, epochs=1)\n",
    "    after = list(model_ngn.parameters())[0].clone().detach().cpu().numpy()\n",
    "    for b, a in zip(before, after):\n",
    "        # Make sure something changed.\n",
    "        assert (b != a).any()\n",
    "\n",
    "\n",
    "def test_targeted_threshold():\n",
    "    dropout_rate = 0.5\n",
    "    threshold = 0.75\n",
    "    weights = torch.arange(100).reshape((10, 10))\n",
    "    weights_out, mask = targeted_neurogenesis(\n",
    "        weights, dropout_rate=dropout_rate, targeted_portion=threshold, is_training=True\n",
    "    )\n",
    "\n",
    "    # targeted population must be below the 7th index\n",
    "    assert torch.all(~mask[7:])\n",
    "\n",
    "\n",
    "#    cnn.train_model(model, dataset, epochs=1, neurogenesis=5, frequency=None,\n",
    "#                turnover=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, shape, epsilon=np.float32(1e-5)):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.tensor(np.ones(shape, dtype='float32')))\n",
    "        self.beta = nn.Parameter(torch.tensor(np.zeros(shape, dtype='float32')))\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = torch.mean(x, (0, 2, 3), keepdim=True)  \n",
    "        std = torch.std(x, (0, 2, 3), keepdim=True)  \n",
    "        x_normalized = (x - mean) / (std**2 + self.epsilon)**0.5  \n",
    "        return self.gamma * x_normalized + self.beta  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(nn.Module):\n",
    "    \"\"\"\n",
    "    http://arxiv.org/abs/1207.0580\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        super().__init__()\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 学習時はdropout_ratio分だけ出力をシャットアウト\n",
    "        if self.training:\n",
    "            self.mask = torch.rand(*x.size()) > self.dropout_ratio\n",
    "            return x * self.mask.to(x.device)\n",
    "        # 推論時は出力に`1.0 - self.dropout_ratio`を乗算することで学習時の出力の大きさに合わせる\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    def __init__(self, filter_shape, function=lambda x: x, stride=(1, 1), padding=0):\n",
    "        super().__init__()\n",
    "        # Heの初期化\n",
    "        # filter_shape: (出力チャンネル数)x(入力チャンネル数)x(縦の次元数)x(横の次元数)\n",
    "        fan_in = filter_shape[1] * filter_shape[2] * filter_shape[3]\n",
    "        fan_out = filter_shape[0] * filter_shape[2] * filter_shape[3]\n",
    "\n",
    "        self.W = nn.Parameter(torch.tensor(rng.normal(\n",
    "                        0,\n",
    "                        np.sqrt(2/fan_in),\n",
    "                        size=filter_shape\n",
    "                    ).astype('float32')))\n",
    "\n",
    "        # バイアスはフィルタごとなので, 出力フィルタ数と同じ次元数\n",
    "        self.b = nn.Parameter(torch.tensor(np.zeros((filter_shape[0]), dtype='float32')))\n",
    "\n",
    "        self.function = function  # 活性化関数\n",
    "        self.stride = stride  # ストライド幅\n",
    "        self.padding = padding  # パディング\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = F.conv2d(x, self.W, bias=self.b, stride=self.stride, padding=self.padding)\n",
    "        return self.function(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling(nn.Module):\n",
    "    def __init__(self, ksize=(2, 2), stride=(2, 2), padding=0):\n",
    "        super().__init__()\n",
    "        self.ksize = ksize  # カーネルサイズ\n",
    "        self.stride = stride  # ストライド幅\n",
    "        self.padding = padding  # パディング\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.avg_pool2d(x, kernel_size=self.ksize, stride=self.stride, padding=self.padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size()[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, function=lambda x: x):\n",
    "        super().__init__()\n",
    "        # Heの初期化\n",
    "        # in_dim: 入力の次元数，out_dim: 出力の次元数\n",
    "\n",
    "        self.W = nn.Parameter(torch.tensor(rng.normal(\n",
    "                        0,\n",
    "                        np.sqrt(2/in_dim),\n",
    "                        size=(in_dim, out_dim)\n",
    "                    ).astype('float32')))\n",
    "\n",
    "        self.b = nn.Parameter(torch.tensor(np.zeros([out_dim]).astype('float32')))\n",
    "        self.function = function\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.function(torch.matmul(x, self.W) + self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(nn.Module):\n",
    "    def __init__(self, function=lambda x: x):\n",
    "        super().__init__()\n",
    "        self.function = function\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.function(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.log(0)によるnanを防ぐ\n",
    "def torch_log(x):\n",
    "    return torch.log(torch.clamp(x, min=1e-10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この下にネットワークを構築する。ニューロン新生を組み込んだものを入れるNgc_CNNとか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#これをモデルにして書くこと。\n",
    "\n",
    "conv_net = nn.Sequential(\n",
    "    Conv((32, 3, 3, 3)),        # 画像の大きさ：32x32x3 -> 30x30x32  # WRITE ME(入出力の画像サイズ）\n",
    "    BatchNorm((32, 30, 30)),\n",
    "    Activation(F.relu),\n",
    "    Pooling((2, 2)),            # 30x30x32 -> 15x15x32  # WRITE ME(入出力の画像サイズ）\n",
    "    Conv((64, 32, 3, 3)),       # 15x15x32 -> 13x13x64  # WRITE ME(入出力の画像サイズ）\n",
    "    BatchNorm((64, 13, 13)),\n",
    "    Activation(F.relu),\n",
    "    Pooling((2, 2)),            # 13x13x64 -> 6x6x64  # WRITE ME(入出力の画像サイズ）\n",
    "    Conv((128, 64, 3, 3)),      # 6x6x64 -> 4x4x128  # WRITE ME(入出力の画像サイズ）\n",
    "    BatchNorm((128, 4, 4)),\n",
    "    Activation(F.relu),\n",
    "    Pooling((2, 2)),            # 4x4x128 -> 2x2x128  # WRITE ME(入出力の画像サイズ）\n",
    "    Flatten(),\n",
    "    Dense(2*2*128, 256, F.relu),  # WRITE ME(in_features)\n",
    "    Dense(256, 10)\n",
    ")\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "n_epochs = 10\n",
    "lr = 0.01\n",
    "device = 'cuda'\n",
    "\n",
    "conv_net.to(device)\n",
    "optimizer = optim.Adam(conv_net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgnCnn(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_size=250,\n",
    "        channels=3,\n",
    "        control=False,\n",
    "        seed=0,\n",
    "        excite=False,\n",
    "        neural_noise=None,\n",
    "    ):\n",
    "        torch.manual_seed(seed)\n",
    "        super().__init__()\n",
    "        # parameters\n",
    "        self.ablate = False\n",
    "        self.dropout = 0\n",
    "        self.channels = channels\n",
    "        self.excite = excite\n",
    "        self.n_new = 0\n",
    "        self.control = False\n",
    "        if self.conZrol:\n",
    "            self.idx_control = np.random.choice(\n",
    "                range(layer_size), size=8, replace=False\n",
    "            )\n",
    "        self.neural_noise = neural_noise\n",
    "\n",
    "        # 3@16x16\n",
    "        self.conv1 = nn.Conv2d(channels, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.pool4 = nn.AvgPool2d(kernel_size=1, stride=1)\n",
    "\n",
    "        self.layer_size = layer_size\n",
    "\n",
    "        self.fc_new_in = nn.ModuleList()\n",
    "        self.fc_new_out = nn.ModuleList()\n",
    "\n",
    "        if self.channels == 3:\n",
    "            self.cnn_output = 64 * 4 * 4\n",
    "        elif self.channels == 1:\n",
    "            self.cnn_output = 64 * 9\n",
    "        # three fully connected layers\n",
    "        self.fcs = nn.ModuleList(\n",
    "            [\n",
    "                nn.Linear(self.cnn_output, self.layer_size),  # 0\n",
    "                nn.Linear(self.layer_size, self.layer_size),  # 1 on dim 2 neurogenesis\n",
    "                nn.Linear(self.layer_size, self.layer_size),  # 2\n",
    "            ]\n",
    "        )\n",
    "        self.fc3 = nn.Linear(self.layer_size, 10, bias=False)\n",
    "\n",
    "    def forward(self, x, extract_layer=None):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        x = x.view(-1, self.cnn_output)\n",
    "\n",
    "        for ix, fc in enumerate(self.fcs):\n",
    "            x = fc(x)\n",
    "            if self.neural_noise is not None and ix == 0 and self.training:\n",
    "                mean, std = self.neural_noise\n",
    "                noise = torch.zeros_like(x, device)\n",
    "                noise = noise.log_normal_(mean=mean, std=std)\n",
    "                x = x * noise\n",
    "            x = F.relu(x)\n",
    "\n",
    "            if self.excite and ix == 1 and self.n_new and self.training:\n",
    "                idx = self.idx_control if self.control else self.idx\n",
    "                excite_mask = torch.ones_like(x)\n",
    "                excite_mask[:, idx] = self.excite\n",
    "                excite_mask.to(device)\n",
    "                x = x * excite_mask\n",
    "\n",
    "            if self.dropout:\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "                x = torch.renorm(x, 1, 1, 3)  # max norm\n",
    "\n",
    "            # for ablation experiments\n",
    "            if self.ablate:\n",
    "                if ix == 1:\n",
    "                    activation_size = x.size()[1]\n",
    "                    if self.ablation_mode == \"random\":\n",
    "                        ablate_size = int(self.ablation_prop * activation_size)\n",
    "                        indices = np.random.choice(\n",
    "                            range(activation_size),\n",
    "                            size=self.size,\n",
    "                            replace=False,\n",
    "                        )\n",
    "                    if self.ablation_mode == \"targetted\":\n",
    "                        indices = self.ablate_indices\n",
    "                    x[:, indices] = 0\n",
    "            if extract_layer == ix:\n",
    "                return x\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def add_new(\n",
    "        self,\n",
    "        p_new=0.01,\n",
    "        replace=True,\n",
    "        targeted_portion=None,\n",
    "        return_idx=False,\n",
    "        layer=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        pnew: float, proportion of hidden layer to add\n",
    "        replace: float,Lina M. Tran  from 0-1 which is the proportion of new neurons that replace old neurons\n",
    "        target: bool, neurons that are lost are randomly chosen, or targetted\n",
    "                based on variance of activity\n",
    "        \"\"\"\n",
    "        # get a copy of current parameters\n",
    "        bias = [ix.bias.detach().clone().cpu() for ix in self.fcs]\n",
    "        current = [ix.weight.detach().clone().cpu() for ix in self.fcs]\n",
    "        if layer == 2:\n",
    "            current_fc3 = self.fc3.weight.detach().clone().cpu()\n",
    "\n",
    "        # how many neurons to add?\n",
    "        if not p_new:\n",
    "            return\n",
    "        # if int given, use this as number of neurons to add\n",
    "        if (p_new % 1) == 0:\n",
    "            n_new = p_new\n",
    "        # if float given, use to calculate number of neurons to add\n",
    "        else:\n",
    "            n_new = int(self.layer_size * p_new)\n",
    "\n",
    "        if targeted_portion is not None:\n",
    "            targ_diff = round(targeted_portion * current[layer].shape[0]) - n_new\n",
    "            if targ_diff <= 0:\n",
    "                n_new = n_new + targ_diff - 3\n",
    "\n",
    "        self.n_new = n_new\n",
    "        n_replace = n_new if replace else 0  # number lost\n",
    "        difference = n_new - n_replace  # net addition or loss\n",
    "        self.layer_size += difference  # final layer size\n",
    "\n",
    "        # reallocate the weights and biases\n",
    "        if replace:\n",
    "            # if some neurons are being removed\n",
    "            if targeted_portion is not None:\n",
    "                try:\n",
    "                    weights, mask = targeted_neurogenesis(\n",
    "                        current[layer], n_replace, targeted_portion, self.training\n",
    "                    )\n",
    "                except ValueError:\n",
    "                    print(\n",
    "                        \"n_replace\",\n",
    "                        n_replace,\n",
    "                        \"targ\",\n",
    "                        targeted_portion * (current[layer].shape[0]),\n",
    "                    )\n",
    "\n",
    "                # if neurons are targetted for removal\n",
    "                idx = np.where(mask)[0]\n",
    "                bias[1] = np.delete(bias[1], idx)\n",
    "                current[layer] = np.delete(current[layer], idx, axis=0)\n",
    "                current[layer + 1] = np.delete(current[layer + 1], idx, axis=1)\n",
    "            else:\n",
    "                # if neurons are randomly chosen for removal\n",
    "                idx = np.random.choice(\n",
    "                    range(current[layer].shape[0]), size=n_replace, replace=False\n",
    "                )\n",
    "\n",
    "                # delete idx neurons from bias and current weights (middle layer)\n",
    "                bias[1] = np.delete(bias[1], idx)\n",
    "                current[layer] = np.delete(current[layer], idx, axis=0)\n",
    "                try:\n",
    "                    current[layer + 1] = np.delete(current[layer + 1], idx, axis=1)\n",
    "                except IndexError:\n",
    "                    current_fc3 = np.delete(current_fc3, idx, axis=1)\n",
    "\n",
    "\n",
    "            self.idx = idx\n",
    "\n",
    "        # create new weight shapes\n",
    "        w_in = torch.Tensor(\n",
    "            self.layer_size,\n",
    "            current[layer].shape[1],\n",
    "        )\n",
    "        b_in = torch.Tensor(self.layer_size)\n",
    "        if layer < 2:\n",
    "            w_out = torch.Tensor(\n",
    "                current[layer + 1].shape[0],\n",
    "                self.layer_size,\n",
    "            )\n",
    "        elif layer == 2:\n",
    "            w_out = torch.Tensor(\n",
    "                current_fc3.shape[0],\n",
    "                self.layer_size,\n",
    "            )\n",
    "\n",
    "        # initialize new weights\n",
    "        nn.init.kaiming_uniform_(w_in, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(w_out, a=math.sqrt(5))\n",
    "\n",
    "        # in bias (out bias unaffected by neurogenesis)\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(w_in)\n",
    "        bound = 1 / math.sqrt(fan_in)\n",
    "        nn.init.uniform_(b_in, -bound, bound)\n",
    "\n",
    "        # put back current bias and weights into newly initiliazed layers\n",
    "        b_in[:-n_new] = bias[1]\n",
    "        w_in[:-n_new, :] = current[layer]\n",
    "        if layer == 2:\n",
    "            w_out[:, :-n_new] = current_fc3\n",
    "        else:\n",
    "            w_out[:, :-n_new] = current[layer + 1]\n",
    "\n",
    "        # create the parameters again\n",
    "        self.fcs[layer].bias = nn.Parameter(b_in)\n",
    "        self.fcs[layer].weight = nn.Parameter(w_in)\n",
    "        if layer == 2:\n",
    "            self.fc3.weight = nn.Parameter(w_out)\n",
    "        else:\n",
    "            self.fcs[layer + 1].weight = nn.Parameter(w_out)\n",
    "\n",
    "        # need to send all the data to GPU again\n",
    "        self.fcs.to(device)\n",
    "        if layer == 2:\n",
    "            self.fc3.to(device)\n",
    "\n",
    "        if return_idx and (n_replace > 0):\n",
    "            return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データロードを定義する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data number:40000, Valid data number: 10000\n"
     ]
    }
   ],
   "source": [
    "trainval_dataset = datasets.CIFAR10('./data/cifar10', train=True, transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "# 前処理を定義\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "trainval_dataset = datasets.CIFAR10('../data/cifar10', train=True, transform=transform)\n",
    "\n",
    "# trainとvalidに分割\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(trainval_dataset, [len(trainval_dataset)-10000, 10000])\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataloader_valid = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"Train data number:{}, Valid data number: {}\".format(len(train_dataset), len(val_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Module.train() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m losses_train \u001b[39m=\u001b[39m []  \u001b[39m# 訓練誤差を格納しておくリスト\u001b[39;00m\n\u001b[1;32m      3\u001b[0m losses_valid \u001b[39m=\u001b[39m []  \u001b[39m# 検証データの誤差を格納しておくリスト\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m NgnCnn\u001b[39m.\u001b[39;49mtrain()  \u001b[39m# 訓練モードにする\u001b[39;00m\n\u001b[1;32m      6\u001b[0m n_train \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m  \u001b[39m# 訓練データ数\u001b[39;00m\n\u001b[1;32m      7\u001b[0m acc_train \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m  \u001b[39m# 訓練データに対する精度\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Module.train() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    losses_train = []  # 訓練誤差を格納しておくリスト\n",
    "    losses_valid = []  # 検証データの誤差を格納しておくリスト\n",
    "\n",
    "    NgnCnn.train()  # 訓練モードにする\n",
    "    n_train = 0  # 訓練データ数\n",
    "    acc_train = 0  # 訓練データに対する精度\n",
    "    for x, t in dataloader_train:\n",
    "        n_train += t.size()[0]\n",
    "\n",
    "        conv_net.zero_grad()  # 勾配の初期化\n",
    "\n",
    "        x = x.to(device)  # テンソルをGPUに移動\n",
    "\n",
    "        t_hot = torch.eye(10)[t]  # 正解ラベルをone-hot vector化\n",
    "\n",
    "        t = t.to(device)\n",
    "        t_hot = t_hot.to(device)  # 正解ラベルとone-hot vectorをそれぞれGPUに移動\n",
    "\n",
    "        y = conv_net.forward(x)  # 順伝播\n",
    "\n",
    "        loss = -(t_hot*torch.log_softmax(y, dim=-1)).sum(axis=1).mean()  # 誤差(クロスエントロピー誤差関数)の計算\n",
    "\n",
    "        loss.backward()  # 誤差の逆伝播\n",
    "\n",
    "        optimizer.step()  # パラメータの更新\n",
    "\n",
    "        pred = y.argmax(1)  # 最大値を取るラベルを予測ラベルとする\n",
    "\n",
    "        acc_train += (pred == t).float().sum().item()\n",
    "        losses_train.append(loss.tolist())\n",
    "\n",
    "    conv_net.eval()  # 評価モードにする\n",
    "    n_val = 0\n",
    "    acc_val = 0\n",
    "    for x, t in dataloader_valid:\n",
    "        n_val += t.size()[0]\n",
    "\n",
    "        x = x.to(device)  # テンソルをGPUに移動\n",
    "\n",
    "        t_hot = torch.eye(10)[t]  # 正解ラベルをone-hot vector化\n",
    "\n",
    "        t = t.to(device)\n",
    "        t_hot = t_hot.to(device)  # 正解ラベルとone-hot vectorをそれぞれGPUに移動\n",
    "\n",
    "        y = conv_net.forward(x)  # 順伝播\n",
    "\n",
    "        loss = -(t_hot*torch.log_softmax(y, dim=-1)).sum(axis=1).mean()  # 誤差(クロスエントロピー誤差関数)の計算\n",
    "\n",
    "        pred = y.argmax(1)  # 最大値を取るラベルを予測ラベルとする\n",
    "\n",
    "        acc_val += (pred == t).float().sum().item()\n",
    "        losses_valid.append(loss.tolist())\n",
    "\n",
    "    print('EPOCH: {}, Train [Loss: {:.3f}, Accuracy: {:.3f}], Valid [Loss: {:.3f}, Accuracy: {:.3f}]'.format(\n",
    "        epoch,\n",
    "        np.mean(losses_train),\n",
    "        acc_train/n_train,\n",
    "        np.mean(losses_valid),\n",
    "        acc_val/n_val\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
